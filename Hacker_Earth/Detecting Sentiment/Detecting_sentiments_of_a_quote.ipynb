{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting sentiments of a quote.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODtxoxdV4AYsXh/ArHevk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srinivas1258/ML-DL/blob/master/Hacker_Earth/Detecting%20Sentiment/Detecting_sentiments_of_a_quote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TegJWjCIzH0u",
        "colab_type": "text"
      },
      "source": [
        "You work as a social media moderator for your firm. Your key responsibility is to tag uploaded content (images) during Pride Month based on its sentiment (positive, negative, or random) and categorize them for internal reference and SEO optimization.\n",
        "\n",
        "Task\n",
        "Your task is to build an engine that combines the concepts of OCR and NLP that accepts a .jpg file as input, extracts the text, if any, and classifies sentiment as positive or negative. If the text sentiment is neutral or an image file does not have any text, then it is classified as random.\n",
        "\n",
        "Data\n",
        "You must use an external dataset to train your model. The attached dataset link contains the sample data of each category [Positive | Negative | Random] and test data.\n",
        "\n",
        "Data files\n",
        "\n",
        "File name\tDescription\n",
        "Test.zip\tContains image files to be classified\n",
        "Sample.zip\tContains sample image files belonging to each category\n",
        "Test.csv\tPredictions file containing indices of test data and a blank target column\n",
        "sample_submission.csv\tSubmission format to be followed for uploading predictions\n",
        "Data description\n",
        "\n",
        "Column name\tDescription\n",
        "\n",
        "\n",
        "Filename\tFile name of test data image\n",
        "\n",
        "Category\tTarget column [values: 'Positive'/'Negative'/'Random']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3RcxVgwy5zf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "32436bd1-1e10-4ed8-f326-e71e912ac6e7"
      },
      "source": [
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 0s (32.3 MB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 2,755 kB of archives.\n",
            "After this operation, 13.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libleptonica-dev amd64 1.75.3-3 [1,308 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libtesseract-dev amd64 4.00~git2288-10f4998a-2 [1,447 kB]\n",
            "Fetched 2,755 kB in 0s (20.7 MB/s)\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "(Reading database ... 144426 files and directories currently installed.)\n",
            "Preparing to unpack .../libleptonica-dev_1.75.3-3_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.75.3-3) ...\n",
            "Selecting previously unselected package libtesseract-dev.\n",
            "Preparing to unpack .../libtesseract-dev_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking libtesseract-dev (4.00~git2288-10f4998a-2) ...\n",
            "Setting up libleptonica-dev (1.75.3-3) ...\n",
            "Setting up libtesseract-dev (4.00~git2288-10f4998a-2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWlIzevRzvjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "757888ac-d05e-434f-ff2e-aabe732b738d"
      },
      "source": [
        "! pip install Pillow\n",
        "! pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/d8/521db389ff0aae32035bfda6ed39cb2c2e28521c47015f6431f07460c50a/pytesseract-0.3.4.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pytesseract) (7.0.0)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.4-py2.py3-none-any.whl size=13431 sha256=b6b0f02363d9ff4e3a89e13eb19daa7d1dcff4d7209c66ec273c2d8464b6f4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/a0/7596d2e0a73cf0aeffd6f6170862c4e73f3763b7827e48691a\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl11tTu25hu9",
        "colab_type": "text"
      },
      "source": [
        "Installing latest esy ocr for text extraction.(pytorch version)\n",
        "\n",
        "[Github link](https://github.com/JaidedAI/EasyOCR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD-HB6f9VPB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "219a1bce-0e8c-4254-85a1-2b90990edf69"
      },
      "source": [
        "!pip install easyocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from easyocr) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from easyocr) (1.4.1)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.6/dist-packages (from easyocr) (0.6.1+cu101)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from easyocr) (0.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from easyocr) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from easyocr) (4.1.2.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from easyocr) (1.5.1+cu101)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr) (2.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr) (2.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->easyocr) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->easyocr) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXTrQeBq2oRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "776f772d-435d-4648-9cf4-7d42cde9f6e1"
      },
      "source": [
        "!pip install git+git://github.com/jaidedai/easyocr.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/jaidedai/easyocr.git\n",
            "  Cloning git://github.com/jaidedai/easyocr.git to /tmp/pip-req-build-eb3vpybu\n",
            "  Running command git clone -q git://github.com/jaidedai/easyocr.git /tmp/pip-req-build-eb3vpybu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (0.6.1+cu101)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (7.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from easyocr==1.1.4) (0.16.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->easyocr==1.1.4) (0.16.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr==1.1.4) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr==1.1.4) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr==1.1.4) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->easyocr==1.1.4) (2.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr==1.1.4) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr==1.1.4) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr==1.1.4) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr==1.1.4) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->easyocr==1.1.4) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr==1.1.4) (1.12.0)\n",
            "Building wheels for collected packages: easyocr\n",
            "  Building wheel for easyocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easyocr: filename=easyocr-1.1.4-cp36-none-any.whl size=20580064 sha256=1b8e016527f14d06c08b4faf4f60f63fe64873d225c370ec464f424334c279b1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-45xagg5q/wheels/e8/77/61/a2618f4b50cdf417e0f3fd201f9814addc89e429bf05103fb3\n",
            "Successfully built easyocr\n",
            "Installing collected packages: easyocr\n",
            "Successfully installed easyocr-1.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9_nRqJIz7yo",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d805f008-db18-4901-f5b9-781d12bf7e1a"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-42720a35-03c5-440f-8830-9609b590f214\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-42720a35-03c5-440f-8830-9609b590f214\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Dataset.zip to Dataset.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwVMAGZ5G8QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mv Test_3277.jpg 'Data Files/Dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IVFxKUF7Fmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1261903e-5361-474f-8292-5a806a610ddc"
      },
      "source": [
        "!unzip Dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Dataset.zip\n",
            "   creating: Dataset/\n",
            "  inflating: Dataset/Test100.jpg     \n",
            "  inflating: Dataset/Test1001.jpg    \n",
            "  inflating: Dataset/Test1012.jpg    \n",
            "  inflating: Dataset/Test1022.jpg    \n",
            "  inflating: Dataset/Test103.jpg     \n",
            "  inflating: Dataset/Test105.jpg     \n",
            "  inflating: Dataset/Test107.jpg     \n",
            "  inflating: Dataset/Test1071.jpg    \n",
            "  inflating: Dataset/Test108.jpg     \n",
            "  inflating: Dataset/Test109.jpg     \n",
            "  inflating: Dataset/Test1122.jpg    \n",
            "  inflating: Dataset/Test113.jpg     \n",
            "  inflating: Dataset/Test114.jpg     \n",
            "  inflating: Dataset/Test1161.jpg    \n",
            "  inflating: Dataset/Test117.jpg     \n",
            "  inflating: Dataset/Test119.jpg     \n",
            "  inflating: Dataset/Test1199.jpg    \n",
            "  inflating: Dataset/Test122.jpg     \n",
            "  inflating: Dataset/Test1229.jpg    \n",
            "  inflating: Dataset/Test1240.jpg    \n",
            "  inflating: Dataset/Test125.jpg     \n",
            "  inflating: Dataset/Test126.jpg     \n",
            "  inflating: Dataset/Test1271.jpg    \n",
            "  inflating: Dataset/Test1279.jpg    \n",
            "  inflating: Dataset/Test128.jpg     \n",
            "  inflating: Dataset/Test129.jpg     \n",
            "  inflating: Dataset/Test1290.jpg    \n",
            "  inflating: Dataset/Test131.jpg     \n",
            "  inflating: Dataset/Test132.jpg     \n",
            "  inflating: Dataset/Test133.jpg     \n",
            "  inflating: Dataset/Test134.jpg     \n",
            "  inflating: Dataset/Test135.jpg     \n",
            "  inflating: Dataset/Test1353.jpg    \n",
            "  inflating: Dataset/Test1359.jpg    \n",
            "  inflating: Dataset/Test136.jpg     \n",
            "  inflating: Dataset/Test140.jpg     \n",
            "  inflating: Dataset/Test141.jpg     \n",
            "  inflating: Dataset/Test143.jpg     \n",
            "  inflating: Dataset/Test144.jpg     \n",
            "  inflating: Dataset/Test145.jpg     \n",
            "  inflating: Dataset/Test149.jpg     \n",
            "  inflating: Dataset/Test151.jpg     \n",
            "  inflating: Dataset/Test152.jpg     \n",
            "  inflating: Dataset/Test154.jpg     \n",
            "  inflating: Dataset/Test158.jpg     \n",
            "  inflating: Dataset/Test160.jpg     \n",
            "  inflating: Dataset/Test161.jpg     \n",
            "  inflating: Dataset/Test1615.jpg    \n",
            "  inflating: Dataset/Test162.jpg     \n",
            "  inflating: Dataset/Test1620.jpg    \n",
            "  inflating: Dataset/Test1634.jpg    \n",
            "  inflating: Dataset/Test164.jpg     \n",
            "  inflating: Dataset/Test1644.jpg    \n",
            "  inflating: Dataset/Test165.jpg     \n",
            "  inflating: Dataset/Test166.jpg     \n",
            "  inflating: Dataset/Test168.jpg     \n",
            "  inflating: Dataset/Test1717.jpg    \n",
            "  inflating: Dataset/Test172.jpg     \n",
            "  inflating: Dataset/Test1724.jpg    \n",
            "  inflating: Dataset/Test173.jpg     \n",
            "  inflating: Dataset/Test1743.jpg    \n",
            "  inflating: Dataset/Test176.jpg     \n",
            "  inflating: Dataset/Test1767.jpg    \n",
            "  inflating: Dataset/Test1776.jpg    \n",
            "  inflating: Dataset/Test1785.jpg    \n",
            "  inflating: Dataset/Test1788.jpg    \n",
            "  inflating: Dataset/Test179.jpg     \n",
            "  inflating: Dataset/Test180.jpg     \n",
            "  inflating: Dataset/Test1803.jpg    \n",
            "  inflating: Dataset/Test181.jpg     \n",
            "  inflating: Dataset/Test1818.jpg    \n",
            "  inflating: Dataset/Test183.jpg     \n",
            "  inflating: Dataset/Test1837.jpg    \n",
            "  inflating: Dataset/Test1856.jpg    \n",
            "  inflating: Dataset/Test187.jpg     \n",
            "  inflating: Dataset/Test1872.jpg    \n",
            "  inflating: Dataset/Test188.jpg     \n",
            "  inflating: Dataset/Test1884.jpg    \n",
            "  inflating: Dataset/Test1885.jpg    \n",
            "  inflating: Dataset/Test1889.jpg    \n",
            "  inflating: Dataset/Test1902.jpg    \n",
            "  inflating: Dataset/Test191.jpg     \n",
            "  inflating: Dataset/Test192.jpg     \n",
            "  inflating: Dataset/Test1923.jpg    \n",
            "  inflating: Dataset/Test1934.jpg    \n",
            "  inflating: Dataset/Test194.jpg     \n",
            "  inflating: Dataset/Test1954.jpg    \n",
            "  inflating: Dataset/Test198.jpg     \n",
            "  inflating: Dataset/Test1992.jpg    \n",
            "  inflating: Dataset/Test2007.jpg    \n",
            "  inflating: Dataset/Test201.jpg     \n",
            "  inflating: Dataset/Test204.jpg     \n",
            "  inflating: Dataset/Test2049.jpg    \n",
            "  inflating: Dataset/Test206.jpg     \n",
            "  inflating: Dataset/Test2068.jpg    \n",
            "  inflating: Dataset/Test209.jpg     \n",
            "  inflating: Dataset/Test211.jpg     \n",
            "  inflating: Dataset/Test2114.jpg    \n",
            "  inflating: Dataset/Test212.jpg     \n",
            "  inflating: Dataset/Test213.jpg     \n",
            "  inflating: Dataset/Test216.jpg     \n",
            "  inflating: Dataset/Test218.jpg     \n",
            "  inflating: Dataset/Test2209.jpg    \n",
            "  inflating: Dataset/Test221.jpg     \n",
            "  inflating: Dataset/Test2235.jpg    \n",
            "  inflating: Dataset/Test225.jpg     \n",
            "  inflating: Dataset/Test227.jpg     \n",
            "  inflating: Dataset/Test228.jpg     \n",
            "  inflating: Dataset/Test2280.jpg    \n",
            "  inflating: Dataset/Test230.jpg     \n",
            "  inflating: Dataset/Test2309.jpg    \n",
            "  inflating: Dataset/Test231.jpg     \n",
            "  inflating: Dataset/Test232.jpg     \n",
            "  inflating: Dataset/Test233.jpg     \n",
            "  inflating: Dataset/Test234.jpg     \n",
            "  inflating: Dataset/Test236.jpg     \n",
            "  inflating: Dataset/Test237.jpg     \n",
            "  inflating: Dataset/Test2371.jpg    \n",
            "  inflating: Dataset/Test238.jpg     \n",
            "  inflating: Dataset/Test239.jpg     \n",
            "  inflating: Dataset/Test240.jpg     \n",
            "  inflating: Dataset/Test2411.jpg    \n",
            "  inflating: Dataset/Test242.jpg     \n",
            "  inflating: Dataset/Test2424.jpg    \n",
            "  inflating: Dataset/Test243.jpg     \n",
            "  inflating: Dataset/Test2434.jpg    \n",
            "  inflating: Dataset/Test244.jpg     \n",
            "  inflating: Dataset/Test245.jpg     \n",
            "  inflating: Dataset/Test2455.jpg    \n",
            "  inflating: Dataset/Test249.jpg     \n",
            "  inflating: Dataset/Test250.jpg     \n",
            "  inflating: Dataset/Test2590.jpg    \n",
            "  inflating: Dataset/Test2616.jpg    \n",
            "  inflating: Dataset/Test2648.jpg    \n",
            "  inflating: Dataset/Test2650.jpg    \n",
            "  inflating: Dataset/Test2743.jpg    \n",
            "  inflating: Dataset/Test2791.jpg    \n",
            "  inflating: Dataset/Test2792.jpg    \n",
            "  inflating: Dataset/Test2870.jpg    \n",
            "  inflating: Dataset/Test2959.jpg    \n",
            "  inflating: Dataset/Test2962.jpg    \n",
            "  inflating: Dataset/Test2977.jpg    \n",
            "  inflating: Dataset/Test3024.jpg    \n",
            "  inflating: Dataset/Test3030.jpg    \n",
            "  inflating: Dataset/Test3118.jpg    \n",
            "  inflating: Dataset/Test3130.jpg    \n",
            "  inflating: Dataset/Test3159.jpg    \n",
            "  inflating: Dataset/Test3177.jpg    \n",
            "  inflating: Dataset/Test3246.jpg    \n",
            "  inflating: Dataset/Test3265.jpg    \n",
            "  inflating: Dataset/Test3277.jpg    \n",
            "  inflating: Dataset/Test3294.jpg    \n",
            "  inflating: Dataset/Test3360.jpg    \n",
            "  inflating: Dataset/Test3512.jpg    \n",
            "  inflating: Dataset/Test3525.jpg    \n",
            "  inflating: Dataset/Test353.jpg     \n",
            "  inflating: Dataset/Test3566.jpg    \n",
            "  inflating: Dataset/Test360.jpg     \n",
            "  inflating: Dataset/Test3625.jpg    \n",
            "  inflating: Dataset/Test3706.jpg    \n",
            "  inflating: Dataset/Test372.jpg     \n",
            "  inflating: Dataset/Test374.jpg     \n",
            "  inflating: Dataset/Test3749.jpg    \n",
            "  inflating: Dataset/Test3750.jpg    \n",
            "  inflating: Dataset/Test378.jpg     \n",
            "  inflating: Dataset/Test3789.jpg    \n",
            "  inflating: Dataset/Test3798.jpg    \n",
            "  inflating: Dataset/Test3827.jpg    \n",
            "  inflating: Dataset/Test3863.jpg    \n",
            "  inflating: Dataset/Test3864.jpg    \n",
            "  inflating: Dataset/Test3893.jpg    \n",
            "  inflating: Dataset/Test3934.jpg    \n",
            "  inflating: Dataset/Test396.jpg     \n",
            "  inflating: Dataset/Test397.jpg     \n",
            "  inflating: Dataset/Test3975.jpg    \n",
            "  inflating: Dataset/Test3985.jpg    \n",
            "  inflating: Dataset/Test400.jpg     \n",
            "  inflating: Dataset/Test417.jpg     \n",
            "  inflating: Dataset/Test422.jpg     \n",
            "  inflating: Dataset/Test440.jpg     \n",
            "  inflating: Dataset/Test449.jpg     \n",
            "  inflating: Dataset/Test482.jpg     \n",
            "  inflating: Dataset/Test489.jpg     \n",
            "  inflating: Dataset/Test490.jpg     \n",
            "  inflating: Dataset/Test498.jpg     \n",
            "  inflating: Dataset/Test516.jpg     \n",
            "  inflating: Dataset/Test519.jpg     \n",
            "  inflating: Dataset/Test522.jpg     \n",
            "  inflating: Dataset/Test523.jpg     \n",
            "  inflating: Dataset/Test526.jpg     \n",
            "  inflating: Dataset/Test527.jpg     \n",
            "  inflating: Dataset/Test533.jpg     \n",
            "  inflating: Dataset/Test536.jpg     \n",
            "  inflating: Dataset/Test545.jpg     \n",
            "  inflating: Dataset/Test579.jpg     \n",
            "  inflating: Dataset/Test581.jpg     \n",
            "  inflating: Dataset/Test587.jpg     \n",
            "  inflating: Dataset/Test606.jpg     \n",
            "  inflating: Dataset/Test612.jpg     \n",
            "  inflating: Dataset/Test614.jpg     \n",
            "  inflating: Dataset/Test624.jpg     \n",
            "  inflating: Dataset/Test644.jpg     \n",
            "  inflating: Dataset/Test660.jpg     \n",
            "  inflating: Dataset/Test661.jpg     \n",
            "  inflating: Dataset/Test665.jpg     \n",
            "  inflating: Dataset/Test668.jpg     \n",
            "  inflating: Dataset/Test677.jpg     \n",
            "  inflating: Dataset/Test715.jpg     \n",
            "  inflating: Dataset/Test751.jpg     \n",
            "  inflating: Dataset/Test757.jpg     \n",
            "  inflating: Dataset/Test760.jpg     \n",
            "  inflating: Dataset/Test761.jpg     \n",
            "  inflating: Dataset/Test778.jpg     \n",
            "  inflating: Dataset/Test779.jpg     \n",
            "  inflating: Dataset/Test789.jpg     \n",
            "  inflating: Dataset/Test803.jpg     \n",
            "  inflating: Dataset/Test811.jpg     \n",
            "  inflating: Dataset/Test812.jpg     \n",
            "  inflating: Dataset/Test824.jpg     \n",
            "  inflating: Dataset/Test833.jpg     \n",
            "  inflating: Dataset/Test834.jpg     \n",
            "  inflating: Dataset/Test835.jpg     \n",
            "  inflating: Dataset/Test837.jpg     \n",
            "  inflating: Dataset/Test843.jpg     \n",
            "  inflating: Dataset/Test859.jpg     \n",
            "  inflating: Dataset/Test861.jpg     \n",
            "  inflating: Dataset/Test884.jpg     \n",
            "  inflating: Dataset/Test890.jpg     \n",
            "  inflating: Dataset/Test903.jpg     \n",
            "  inflating: Dataset/Test912.jpg     \n",
            "  inflating: Dataset/Test929.jpg     \n",
            "  inflating: Dataset/Test937.jpg     \n",
            "  inflating: Dataset/Test941.jpg     \n",
            "  inflating: Dataset/Test942.jpg     \n",
            "  inflating: Dataset/Test945.jpg     \n",
            "  inflating: Dataset/Test946.jpg     \n",
            "  inflating: Dataset/Test957.jpg     \n",
            "  inflating: Dataset/Test979.jpg     \n",
            "  inflating: Dataset/Test993.jpg     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csdl6MhesSAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e4ac7f3-59ec-4d9d-a03d-e431b4243c29"
      },
      "source": [
        "ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/  Dataset.zip  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljRhKsbu6Ut2",
        "colab_type": "text"
      },
      "source": [
        "**text extraction using pytorch version easy ocr**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2KNaLJFTsxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "87dd227f-7cda-4fa2-920b-3295bdbe8726"
      },
      "source": [
        "import easyocr\n",
        "reader = easyocr.Reader(['en'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading detection model, please wait\n",
            "Download complete\n",
            "Downloading recognition model, please wait\n",
            "Download complete\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVd_1jmH6UHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os                                # converting images into text using pytesseract (some filters are applied before for better enhancement)\n",
        "import pytesseract as pyt\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "text_list=[]\n",
        "filenames=[]\n",
        "images_path=glob.iglob('Dataset/*.jpg')\n",
        "\n",
        "\n",
        "for file_name in images_path:\n",
        "  text = reader.readtext(file_name,detail=0)\n",
        "  text_list.append(text)\n",
        "  filenames.append(file_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl5v18Ja-m3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "32fa8377-5b7c-48f0-faee-208c813ae5fd"
      },
      "source": [
        "'''# for detecting text in blur images\n",
        "import cv2\n",
        "\n",
        "# file_name='Data Files/Dataset/Test126.jpg'\n",
        "file_name='Test100.jpg'\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# plt.imshow(plt.imread(file_name))\n",
        "# import cv2\n",
        "\n",
        "image = cv2.imread(file_name, 0)\n",
        "imgBlur = cv2.GaussianBlur(image, (9, 9), 0)\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "imgTH = cv2.morphologyEx(imgBlur, cv2.MORPH_TOPHAT, kernel)\n",
        "_, imgBin = cv2.threshold(imgTH, 0, 250, cv2.THRESH_OTSU)\n",
        "\n",
        "imgdil = cv2.dilate(imgBin, kernel)\n",
        "_, imgBin_Inv = cv2.threshold(imgdil, 0, 250, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "# cv2_imshow( image)\n",
        "# cv2_imshow( imgBin)\n",
        "# cv2_imshow( imgdil)\n",
        "cv2_imshow( imgBin_Inv)\n",
        "\n",
        "cv2.imwrite('./output.png', imgBin_Inv)\n",
        "cv2.waitKey(0)\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image = cv2.imread(file_name)\n",
        "sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "sharpen = cv2.filter2D(image, -1, sharpen_kernel)\n",
        "\n",
        "cv2_imshow(sharpen)\n",
        "cv2.waitKey()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"# for detecting text in blur images\\nimport cv2\\n\\n# file_name='Data Files/Dataset/Test126.jpg'\\nfile_name='Test100.jpg'\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom google.colab.patches import cv2_imshow\\n\\n# plt.imshow(plt.imread(file_name))\\n# import cv2\\n\\nimage = cv2.imread(file_name, 0)\\nimgBlur = cv2.GaussianBlur(image, (9, 9), 0)\\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\\nimgTH = cv2.morphologyEx(imgBlur, cv2.MORPH_TOPHAT, kernel)\\n_, imgBin = cv2.threshold(imgTH, 0, 250, cv2.THRESH_OTSU)\\n\\nimgdil = cv2.dilate(imgBin, kernel)\\n_, imgBin_Inv = cv2.threshold(imgdil, 0, 250, cv2.THRESH_BINARY_INV)\\n\\n# cv2_imshow( image)\\n# cv2_imshow( imgBin)\\n# cv2_imshow( imgdil)\\ncv2_imshow( imgBin_Inv)\\n\\ncv2.imwrite('./output.png', imgBin_Inv)\\ncv2.waitKey(0)\\nimport cv2\\nimport numpy as np\\n\\nimage = cv2.imread(file_name)\\nsharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\\nsharpen = cv2.filter2D(image, -1, sharpen_kernel)\\n\\ncv2_imshow(sharpen)\\ncv2.waitKey()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTFB_D-yBgO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.imshow(plt.imread(file_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAfagy4a-9ME",
        "colab_type": "text"
      },
      "source": [
        "Using tensorflow pytesseract to extract text (cons: noise background,hand writing,text_size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmmi7ap07L3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "3b060d6d-9bce-4feb-d360-69bbb55683ce"
      },
      "source": [
        "'''import os                                # converting images into text using pytesseract (some filters are applied before for better enhancement)\n",
        "import pytesseract as pyt\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "text_list=[]\n",
        "filenames=[]\n",
        "images_path=glob.iglob('Dataset/*.jpg')\n",
        "\n",
        "for file_name in images_path:\n",
        "\n",
        "\n",
        "  image = cv2.imread(file_name,0)\n",
        "  # dst = cv2.fastNlMeansDenoisingColored(image,None,5,10,7,21)\n",
        "  dst1=cv2.fastNlMeansDenoising(image,None,5,7,21)  # denoising filter to remove noise in image\n",
        "  # (thresh, dst2) = cv2.threshold(dst1, 127, 255, cv2.THRESH_BINARY)\n",
        "  # gray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
        "  # scale_percent = 120 # percent of original size\n",
        "  # scale_percent=150\n",
        "  # width = int(image.shape[1]*scale_percent / 100)\n",
        "  # height = int(image.shape[0]*scale_percent / 100)\n",
        "  # dim = (width, height)\n",
        "# resize image\n",
        "  # resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "  sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) # sharp the letters in image\n",
        "  sharpen = cv2.filter2D(dst1, -1, sharpen_kernel)\n",
        "  filename1='sharp.jpg'\n",
        "  cv2.imwrite(filename1, sharpen)  # creating file after preprocessing\n",
        "  # \n",
        "  # cv2_imshow(sharpen)\n",
        "  #print(file_name)\n",
        "  text = pyt.image_to_string(Image.open(filename1)) # converting image to text\n",
        "  text_list.append(text)  # appending to list \n",
        "  filenames.append(file_name)\n",
        "  # print(text,'\\n')\n",
        "  '''\n",
        "# '''for file,text in zip(filenames,text_list):\n",
        "  #  print(file , text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"import os                                # converting images into text using pytesseract (some filters are applied before for better enhancement)\\nimport pytesseract as pyt\\nfrom PIL import Image\\nimport matplotlib.image as mpimg\\nfrom google.colab.patches import cv2_imshow\\nimport glob\\nimport pandas as pd\\nimport cv2\\nimport numpy as np\\n\\ntext_list=[]\\nfilenames=[]\\nimages_path=glob.iglob('Dataset/*.jpg')\\n\\nfor file_name in images_path:\\n\\n\\n  image = cv2.imread(file_name,0)\\n  # dst = cv2.fastNlMeansDenoisingColored(image,None,5,10,7,21)\\n  dst1=cv2.fastNlMeansDenoising(image,None,5,7,21)  # denoising filter to remove noise in image\\n  # (thresh, dst2) = cv2.threshold(dst1, 127, 255, cv2.THRESH_BINARY)\\n  # gray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\\n  # scale_percent = 120 # percent of original size\\n  # scale_percent=150\\n  # width = int(image.shape[1]*scale_percent / 100)\\n  # height = int(image.shape[0]*scale_percent / 100)\\n  # dim = (width, height)\\n# resize image\\n  # resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\\n  sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) # sharp the letters in image\\n  sharpen = cv2.filter2D(dst1, -1, sharpen_kernel)\\n  filename1='sharp.jpg'\\n  cv2.imwrite(filename1, sharpen)  # creating file after preprocessing\\n  # \\n  # cv2_imshow(sharpen)\\n  #print(file_name)\\n  text = pyt.image_to_string(Image.open(filename1)) # converting image to text\\n  text_list.append(text)  # appending to list \\n  filenames.append(file_name)\\n  # print(text,'\\n')\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoZw9xZKChn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1=pd.DataFrame(filenames,columns=['filename'])# creating dataframe\n",
        "data1.insert(1, \"text\", text_list, True) # inserting text column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC3-yBK95EGB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ae542891-ac5d-4426-82b9-76d8e394a0c9"
      },
      "source": [
        "\n",
        "# data1=pd.DataFrame(filenames,columns=['filename'])# creating dataframe\n",
        "# data1.insert(1, \"text\", text_list, True) # adding text data to dataframe\n",
        "\n",
        "data1.filename=data1.filename.str[8:] # slicing filename \n",
        "\n",
        "dataset=data1[:] # copying dataframe for safety\n",
        "dataset.head() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test128.jpg</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test240.jpg</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test751.jpg</td>\n",
              "      <td>[loses$, slo@]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test100.jpg</td>\n",
              "      <td>[Married, uith, PRIDE]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test533.jpg</td>\n",
              "      <td>[Mretahen, drnial, Leinactir, ondvaoowz, Caugh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      filename                                               text\n",
              "0  Test128.jpg                                                 []\n",
              "1  Test240.jpg                                                 []\n",
              "2  Test751.jpg                                     [loses$, slo@]\n",
              "3  Test100.jpg                             [Married, uith, PRIDE]\n",
              "4  Test533.jpg  [Mretahen, drnial, Leinactir, ondvaoowz, Caugh..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZbmIQenK8z6",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1b68aed4-093e-4368-e703-87162428014c"
      },
      "source": [
        "# dataset.to_csv('easy_ocr.csv')\n",
        "from google.colab import files\n",
        "f=files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8f54a492-866d-4f65-adbe-528ae53cba7e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8f54a492-866d-4f65-adbe-528ae53cba7e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving processed_batch (1).csv to processed_batch (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOUEeT9jw8Zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4019793b-e730-4393-abce-5b778bd182c0"
      },
      "source": [
        "train2=pd.read_csv('processed_batch (1).csv')\n",
        "train2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>clean_sentence</th>\n",
              "      <th>Classification</th>\n",
              "      <th>Confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test751.jpg</td>\n",
              "      <td>loses  slo</td>\n",
              "      <td>Negative</td>\n",
              "      <td>0.795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test100.jpg</td>\n",
              "      <td>married uith pride</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test533.jpg</td>\n",
              "      <td>mretahen drnial leinactir ondvaoowz caughi sha...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test3827.jpg</td>\n",
              "      <td>being gayis nota crime anditis nota sin  stop ...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>0.775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test211.jpg</td>\n",
              "      <td>hat</td>\n",
              "      <td>Negative</td>\n",
              "      <td>0.674</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       filename  ... Confidence\n",
              "0   Test751.jpg  ...      0.795\n",
              "1   Test100.jpg  ...      0.699\n",
              "2   Test533.jpg  ...      0.673\n",
              "3  Test3827.jpg  ...      0.775\n",
              "4   Test211.jpg  ...      0.674\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V0zvcZYQ6dD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4ff8fd9f-53c0-447a-bf78-13d0e6a2f833"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQW7cWQJG-ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8eacedae-724c-4d1e-92c5-87602d0842f8"
      },
      "source": [
        "'''from bs4 import BeautifulSoup\n",
        "import re\n",
        "reviews=[]\n",
        "for sent in (dataset['text']):\n",
        "        \n",
        "        #remove html content\n",
        "        review_text = BeautifulSoup(sent).get_text()\n",
        "        \n",
        "        #remove non-alphabetic characters\n",
        "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "        reviews.append(review_text)\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'from bs4 import BeautifulSoup\\nimport re\\nreviews=[]\\nfor sent in (dataset[\\'text\\']):\\n        \\n        #remove html content\\n        review_text = BeautifulSoup(sent).get_text()\\n        \\n        #remove non-alphabetic characters\\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\\n        reviews.append(review_text)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTZzeEywJGNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset.insert(2,'clean_text',reviews,True)\n",
        "# dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjjcwT79K7FR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a6e111ad-3a31-4f07-a4ed-e60c6905a67a"
      },
      "source": [
        "dataset['length']=dataset['text'].str.len() # finding string length to filtering\n",
        "text_data=dataset[dataset['length']!=0]  # finding text data separately\n",
        "text_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test751.jpg</td>\n",
              "      <td>[loses$, slo@]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test100.jpg</td>\n",
              "      <td>[Married, uith, PRIDE]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test533.jpg</td>\n",
              "      <td>[Mretahen, drnial, Leinactir, ondvaoowz, Caugh...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Test3827.jpg</td>\n",
              "      <td>[Being, gayis nota crime, anditis nota sin. St...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Test211.jpg</td>\n",
              "      <td>[HAT]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       filename                                               text  length\n",
              "2   Test751.jpg                                     [loses$, slo@]       2\n",
              "3   Test100.jpg                             [Married, uith, PRIDE]       3\n",
              "4   Test533.jpg  [Mretahen, drnial, Leinactir, ondvaoowz, Caugh...       8\n",
              "6  Test3827.jpg  [Being, gayis nota crime, anditis nota sin. St...      13\n",
              "7   Test211.jpg                                              [HAT]       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpwNIxknRMIz",
        "colab_type": "text"
      },
      "source": [
        "!nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UMg-XGANUcl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "dc9541ce-7e01-40c4-d3ca-580f3aafadef"
      },
      "source": [
        "random_data=dataset[dataset['length']==0]\n",
        "print(random_data.shape)\n",
        "random_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test128.jpg</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test240.jpg</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Test160.jpg</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Test173.jpg</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Test212.jpg</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      filename text  length\n",
              "0  Test128.jpg   []       0\n",
              "1  Test240.jpg   []       0\n",
              "5  Test160.jpg   []       0\n",
              "8  Test173.jpg   []       0\n",
              "9  Test212.jpg   []       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pbKRPGiU_qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsjNJBHQZu6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "92af024f-0029-48ac-ab12-2d36adc51863"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKycnp1LI7-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0f692ecf-847b-4d10-8887-b41032eadadc"
      },
      "source": [
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))\n",
        "# import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def clean_sentences(df):\n",
        "    reviews = []\n",
        "\n",
        "    for sent in (df['text']):\n",
        "        \n",
        "        #remove html content\n",
        "        review_text = BeautifulSoup(sent).get_text()\n",
        "        \n",
        "        #remove non-alphabetic characters\n",
        "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "        review_text=review_text.lower()\n",
        "        #tokenize the sentences\n",
        "        # words = word_tokenize(review_text.lower())\n",
        "        \n",
        "        # stops = set(stopwords.words(\"english\"))                  \n",
        "    # \n",
        "        # 5. Remove stop words\n",
        "        # meaningful_words = [w for w in words if not w in stops]\n",
        "    \n",
        "        #lemmatize each word to its lemma\n",
        "        # lemma_words = [lemmatizer.lemmatize(i) for i in meaningful_words]\n",
        "    \n",
        "        reviews.append(review_text)\n",
        "\n",
        "    return(reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuCSKx7vAZ8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3e10f17e-60b0-41db-8b3d-52f2723b2ce0"
      },
      "source": [
        "text_data['text'] = [','.join(map(str, l)) for l in text_data['text']]\n",
        "text_data['text']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2                                            loses$,slo@\n",
              "3                                     Married,uith,PRIDE\n",
              "4      Mretahen,drnial,Leinactir,ondvaoowz,Caughi sha...\n",
              "6      Being,gayis nota crime,anditis nota sin. Stop ...\n",
              "7                                                    HAT\n",
              "                             ...                        \n",
              "234                                           PROUD,ll8l\n",
              "235                  Notli?3,nill nok,ealess,go,rgla,M-~\n",
              "236                                          BE,zsuegece\n",
              "237                     o~,66683,a~s,celceve&,Mese Amelo\n",
              "238    THERE'S NOTHING,WRONG WITH YOU.,THERE'S A LOT,...\n",
              "Name: text, Length: 179, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iBpgiieDAnN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "207b5c0d-4689-481c-886d-aaec3ebdd4e4"
      },
      "source": [
        "for i in text_data['text']:\n",
        "  print (i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loses$,slo@\n",
            "Married,uith,PRIDE\n",
            "Mretahen,drnial,Leinactir,ondvaoowz,Caughi shadovshoning,miycubronerioudlwao,feternineaio dolzainadiero,naturafasine\n",
            "Being,gayis nota crime,anditis nota sin. Stop using,Godto justifyyour prejudice.,Religion is aboutlovingone,another You rejust looking,foran excuse tohate.,Being,Gay,& Proud Quotes,geckoandfly.,wwu7U,com\n",
            "HAT\n",
            "Saluinaovvoring ar,lalentzd,zyen nere,qain,noubl,Iibian,tosgnanol2,oucan,cfchins,,lalding,n@,Moll,Gno\n",
            "Ts rusan unsesla[3,a41,noe amolly blkelea byune,seatus quo Is repulsve\n",
            "GT\n",
            "BE,ILuRSHE,naoriyinaly,alwu,wonl ore thu,~cO>4.\n",
            "for,LGBT\n",
            "5 BESTGAY,QUOTES,WWW.GD BL0 G,NE T\n",
            "Google,BeoVor,naans,booontaaa4,nornosanunny,boasenseaa3,aborlon,bor,armartgg,,t6\n",
            "uguu5cI^a!5MIt0,snoutonuingcytntDa,Datgngr,uM,nlnu,st5jntiutngn3CulDG1,nru,nyegoncGrnseCI,mennnso urnnrnu,Guntag,cilNuucurrnaM\n",
            "If Haryy Poller,pught us anylhing,It's that no one,should live in a closet\n",
            "HATE,Ir HAs CAUSEDALOT,Or TnouLLusN THIS,wOHLD, HUTI5 HAS,aoTSOLVED OE VET,MAY^ AvGELou\n",
            "LOVE,IS TOO BEAUTIIL,TO BE HIDDEIN,IN THE CLOSET\n",
            "DL:STGAY,QuOTES,\n",
            "locsy,toc\n",
            "Wo ms scanl,gelect3,me1,ne mest,t,or,be eelectee,Mase,Agela\n",
            "Burst dovn those,closet doors once,and for all, and,stand up and,start to fight.,Harvey Mill,Gsnalmnaci corn\n",
            "Do yu realize whar you,when you say,sa,That's so gayl\"?,GAY\",HOr,Mrcn,cno0,no1.,tobo\n",
            "you Oppose gay rights,So,beCause Of the Bible?,try,Mnless,you also,to outlaw,Shrime Cocktails ILev,11:9),2+:161,Cursing IL,ev,Deut.22:51,Women s jeans,16:51,Arrogance IProv.,-Lying IProv.,12:22),Bacon ILev. 11:7,Aclultery,Deut. 22:231,Working,Sunday INum. 15:32I,On,please Shut the hell upl\n",
            "oooo Verizon,LTE,10:47 AM,34%,X,Tweet,-168,anything negative,apologize,for,r,Ive,said towards gays and for that,matter anyone. | was young,,ignorant, stupid, and in a bad,place. I've moved on and learned,from my mistakes and,am so,truly sorry to anyone,have,have nothing against,offended.,anything,promotes,that,anyone,or,Sorry,equality.,again,W,E,R,T,P,K,S,H,D,F,J,A,z,X,B,N,M,0,#,123,space\n",
            "t\n",
            "diamirer rlrarnisorie,mra,,,\n",
            ",8,Gvally,,otve.,,pnie.,,\n",
            "33,6,qouidel,drcamstimc com,|0 145823917 @ Barrirret\n",
            "The Biblical Case in,Support of Same-Sex,Relationships,()AND,G,C^,THE,A4 Y,CHRISTIAN,MATTHEW,VINES\n",
            "HATE,IT HAS CAUSED A LOT,OF PROBLEMS IN THIS,WORLD, BUIT IS HAS,NOT SOLVED ONE YET.,MAYA ANGELOu\n",
            "PPeople dont know ifim gay straight or,an alien from outer space.. its funny',Gerard Wag\n",
            "SDuNDS,Giy,IM IN\n",
            "zol,\"8}=51nHHT\",AY,STE,THEla]gw\n",
            "KEEP CALM,AND LOVE,WHO YoU WANT\n",
            "ITHINKA,66,MARRIAGE IS AS,A MARRIAGE HAS,ALWAVS BEEN,,6,BETWEENA MAN,AND A WOMAN\",MLUey PDOMAMCLIIVTDI, 2Db0,H#TBT\n",
            "Turaeayrerou ram anrngnnlor,mnnr,aalern irn oarlanlmnnr,ce\n",
            "Memll,NOTBE,ess`,y,ERAED\n",
            "irr,Zlrlr,authol  lr MLiy: lln luoT nihl43,cing,duilengiailar,inlensuly,Riehinltt,11,33,,ranolwhatfbovtunn,0114)711~1,711;14111,mnsralisvtalkalnat,1,,,Instjsildlnnin,rllal,ml\n",
            "Being gayis like being,lefthanded. Some people,,are most people arent,and nobodyreallyknows,why.,Its notrightorwong,,itsjustthe waythings are!,Being,Gay,& Proud Quotes,geckoandfly.,wwu7U,com\n",
            "cny cudiunHis suavIViO,nciVi,son,IG,IOXNCE,HOM0,QulC,succis3,GN,cnude,,ofrOSIIIyL,mndG5.\n",
            "mocte,goy,dosce,ng3,,vomlne,ool,evel,oos2eo,mzveir,annrsa,ord>5,urego\n",
            ",,LOV,,is,aeve,,,\n",
            "The pressures on gay teens can,be overwhelming-to keep secrets,,tell lies. deny who,are. and try,you,to be who you're not. Remember:,you are special and worth being,cared about. loved. and accepted,just as you are. Never. ever let,anyone convince you otherwise,Aler Sancher Aulhor\n",
            "IF I COULD HAVE,CHOSEN TO BE,GAY OR STRAIGHT,,I THINKI WOULD,HAVE SIMPLY,CHOSEN TO BE,HAPPY,0,k,hlrp\n",
            "eo msslest,cartgo p,Tm,becalse,caat3,o,me.,Mase,Anelo\n",
            "I\n",
            "WDODUD-oD=DA,wAis,BenIs TN{VS MAT,@pe,Dlade LnIs IyJy,@DR,Gkc Sl~  len,T mpte Icto ml,gM,in,ay\n",
            "IFFncoUULp HAVE,CMOSEN TO BE,,,,nAVErsmmPLY,CHOSEN TO BE,HAPPY\n",
            "Qwialso,BAUIIFUI,a,fotrrbbN,IT CIOST\n",
            ",,tragedy,in their,some,lives, but,Tve,onlyhad,tragic haircuts and\n",
            "Life's a game.,Play,to win.\n",
            "vality.,,avve.,pile.\n",
            "Jason J,0,jaybs121|4|,Added Me,Everyone add my new,Snapchat,or comment,names below. III be posting,positive quotes every,day,Igbtpride loveislove Igbt trans,transmatter transgender,transgirl transmale lesbian,lesbiansex gaypride gay bi,bisexualpride bisexual bipride,asexual asexualpride,pansexualpride pansexual,protecttranskids,swag ftm,pride,mtf\n",
            "nunal,aaulaen8t6lue fostananuuusnunaieuu,T,majny lslesti 4P nraal tne tnuT semimanay,Irun novsoaunihjs5alv,slsin tuntesuofslav3 tees ls,alajelrile negouse tanray4,e,alu3u3,itelnlT,,mnneinan,fuuca,nse,,Acie aiun lalu,fdusunn,mz sstknssetiestile t\"rsi,ori,,ausu,fevurate natray lln srneval,ik7ikefrol,drnvalNls^,uuTreosoy seaelae Nr.,Atilyauoue4 uin nasaulalsulat avuno,reanrn,tahAltnvanlurz unugsva lsutas tlsytusnov,LuuTleni nannulh luu,ee,1#1,ale,oylolrirn,nan8,i6,,n,7,m suenidsme retersoilgsure fuuayalotunnfsrnmun,m,a4s,fuautun,ncdculnn nossiell ne usaauldalea olue [b5 laeuo,fingsin,rlsuear,rs43,annteigunrne,fouasal,Inarruraie ntraallnlannet,regak1e57671,ioknso,natnn0r,Tyika Nsstinlskal^ebunemanonesnsllnoheleikusulte2,na\" Wion wa Inuylds_. tutalr talsjasmlt\"steueyael,utugi lunir\"1n#42,TR,\"\n",
            "So you Oppose say Tigits,because of the Bible?,to outlaw,try,unless you also,i91,cooikal3IL1,17161,nvy,3,Irrgs,3,tero4,Tisli,nceo,acer;,IDvu. lJI,Merrn,1531!,a,please shut the hell u!\n",
            "ITHINK BEING GAVISA BLESSING,,,ANDITSSOMETHING,IAMTHANFUL FOR EVERVSINGLEDIV,D[RS,,nr,C0,P,0,t,\n",
            "ONNE DAY,WE WONT HAVE TO,ICOME OUT OF,THE CLOSET',WELL JUST SAY,WE ARE IN LOV=,AND THAT WILL,ALL THAT MATT\n",
            "If Harty Poller,pughl us onylhing,irsunalnojois,aonlalislm ajdanart\n",
            "BE,YuRSHE,naorinincly,al0,wotk more thar,a cOP1:\n",
            "Ge,Find oul who you are,land ba thal p arson-,Thal s anal your soul,was put on unis earuh,to bs ind tnal trulh,tve tnat uruln and,evannning slse,unil core,55\n",
            "If Harty Poller,onylhing,pught,us,irreuhal no onie,shoula liin a dosil\n",
            "ACCEPANCE,OF YOURSELF,ISEARMORE,IMPORTANT,THAN,ACCEPTANCE,inoM orHERS.\n",
            "cci,you can,you re halfivay,Z.,r. KaasuL1\n",
            "KEEP,CALM,BECAUSEIT'S OKAY,TO BE,GAY\n",
            "I,)\n",
            "Yov ULLoT HAVL,rolnut tit,hrD,tenilve,FIFD.\n",
            "3\".117?0,410,tralh7\n",
            "GG,99,Top,Pro & Con,Quotes\n",
            "l am nol liece,while anywoman,is unliee, even,when her shackles,are very dillerent,liom my,Ownl.,AUDRE LORDE,GH\n",
            "Do you realize what you,say. when you say,Ihat's so gay!\"?,The way,has,ddays,is usednowa-,gay,a,negative connotation. Which it should not!,GAY,means brighily colored happy and or homosoxual,HOr,AMMoYIMG. sruPID. FooLISH. PATHEIIC.,auplenous.RnAilouneunlous.,,,,WEAK. Docus. niDiculous.,FnIvolous. HIlDISH.,nusunD AwKWAnD.,on,FnEAKY,CHOOSEA WQRD WICH MEANS,WHAT YOU MGANIO SAY.,IIS ONLY IHE SMART IHING IO DO.\n",
            "YOU SAY,'BE STFIGHT',I SAY,TASTE,THE RAINBOW\n",
            "Ln,dfrai!,nof,of,L,uio,e\n",
            "99\n",
            "IAMGAF,IAMSTRAGHE:,IAMEESHIAN,IAMHISEXUAE.,IAMFRANSGENDERED.,IAM HUMAN.\n",
            "IE,m,,ess,a,to w,nores,will,kn,nver,m,La amei9,can Le,zou,M>,Anela\n",
            "We are powerful,because we have,survived.,AUDRE LORDE\n",
            "BE,HA\n",
            ":}\n",
            "\"Shine yur soulwith the same,humility,egoless,as the rainbow andno matter where you go,in this worddorthe nert, love wilfindyou,,attendyou audbless you,Aberhani,Grom Jouneythrughihe Pouerofihe Ranbou,Kabvu 8an bySara L\n",
            "Tatergreram,dnallaar lrn nanllya,fnn\n",
            "Being,gayis nota crime,anditis nota sin. Stop using,Godto justifyyour prejudice.,Religion is aboutlovingone,another You rejust looking,foran excuse tohate.,Being,Gay,& Proud Quotes,geckoandfly.,wwu7U,com\n",
            "B,becter togtl,gts\n",
            "\"Onstage,,devil. But Im,hardlya,Lam,a,social reject.,Hercarg,Freddle,naelamaa ian\"aranaleno\n",
            "Addidle\n",
            "cleliene,you can,@,you' re halfway,Zee.,T. ROOSEVELT\n",
            "\"EVEN IN THE BIBLE WE CANREAD NA,ANG BABAE, DAPAT MAGSUOT NA PAMBABAE AT,ANG LALAKI, MAGSUOT NG PANLALAKI.,THATS WHATI BELIEVE\"\n",
            "go,There s nothing,wrong wirh you.,Theres a lof wrong,wirh the world you,live in,cirs Coller\n",
            "GC,Happiness,is whenwhat,youthink,,whatyou,say,and,what,you,doarein,harmony,95,inopungandpouavequolca com,Inspiring and Positive Quotes,<3\n",
            "nin nint lrii,hilsirsv\"ini,muolru\"t.007:00,vliru lir sl ll`>,ariyrrylillirinl,Iriiiniyii.,nueoane0e0\n",
            "NOTHINC,WOULD PE,THE SPME,E_7OUD!D,NOTEXIST\n",
            "myamrnro,Ioanlosurlnsndl:,seren,no,Coalnaa],a3,Calo,44,9s4ol7\n",
            "MARRIAGE,!S,ABOUT,LOy,NOTCENDER\n",
            "Being gayis like being,lefthanded. Some people,,are most people arent,and nobodyreallyknows,why.,Its notrightorwong,,itsjustthe waythings are!,Being,Gay,& Proud Quotes,geckoandfly.,wwu7U,com\n",
            "GODAND,GAY,THE,CHRISTIAN,MATTHEIV VIES\n",
            "Gn<,me,eMe,re[o,nar,nu,,,cr8,,C^\n",
            "Anti LGBT: States that have laws restricting teachers and stafffrom,talking about LGBT issues at school,alabima,Ailona,toulstana,Mlssissippl,Ouanoma,Soulh Carollna,Tna3,Urah\n",
            "Anti Bullying: States that have laws prohibiting bullying of students on,the bases ofsexual orientation and gender identity,Ailansas,Calllornla,Colorido,Connedticut,Mlllnols,lon,Miln,Maryland,Massacnusetts,Minnesot4,Mevida,Men Mampsnlre,Menlerssy,Msn lorl,Norih Carolina,Oregon,Phodelsland,Vermont,Washington,Washington, DC\n",
            "LOVE,Gve\n",
            "jefeeleern,6aa],Maureu2lse,myusng tael,f,ram,,liraye,mol2]222_,funanne,tazs alueeuna,ae,nGala5eenelunue2l12]#024|00,Dalna mnuae rsp2lalneIl82,e501,ane]6\n",
            "We ore powertul,becouse we hove,sur vived,r\n",
            "m,,,~r~\n",
            "Alotofparents will,do anythingfor their,kids exceptletthem,be themselves.,Being,Gay,& Proud Quotes,o,geckoandfly.,com,ww7u\n",
            "lo belolroid ls,co behove Os,che erurh were,noe crue.\n",
            "korned,composoion lrom,bcing discrminotcd,ogoinst. Everyening,bod uhovs,hopoeneo to,hos Couohe,me comoossons,\n",
            "&ozel9e,o,,c6~~#?9,auae,Aelo\n",
            "you re not out and proud, yours,urte,outannabestmontn,ine,Don,spendanolner,hiding,closet,andsnamne,ou donina,for,roudeserv,and your trib,naitingto,walcomo vouhome\n",
            "Claimingthat someone,else's marniage is against,being,your religion islike,angrywth someone for,eatinga donutwhen,you're on a diet.,Being,Gay,& Proud Quotes,geckoandfly.,wwwU.,com\n",
            "Ofcourse gaymen,dress well. Theydidnt,spend all thattime in,the closet doing nothing.,Being,Gay,& Proud Quotes,www geckoandfly.com\n",
            "SOUNDS,GA_,IM IN,ee\n",
            "IGOIEAISOFIILIIIIGSTORICS\n",
            "CC,Find out who you are,and be that person.,That's what your soul,on this earth,was,put,to be. Find that truth,,live that truth, and,everything else,will come.,Ellen,99\n",
            "Iwish more teachers,could elaborate on it,ILGBTQ topics] and talk,about it more, instead of,like, two sentences and,then dismiss the subject.,ELAINA,in Wnat Do You Know?,Sulo Twelve Year Olds Tal,AbouttGarO Topics,Welcoming Schools Film)\n",
            "cRu,8\n",
            "learned,compassion from,being discriminated,against. Everything,bad that's ever,happened to,me has taught,compassion.,me,DEGENERES,ELLEN\n",
            ",,6#o@),ncalaiaie lanna,iriserda,m0,\n",
            "r0,jurnu;,re\n",
            "We are legal\"\",Hilty,Inall,states\"\n",
            "islongjss uno Esrerlabon,cinoln nastales,stonen,beleg,belemens sumi as ennal,fuur ang Suna Bin ula nol,nddlnbronesaalsydors,aa04\n",
            "Gay,pride or LGBT pride is the positive stance against,discrimination and violence toward lesbian, gay,,bisexual, and transgender (LGBT) people to promote,their self-affirmation, dignity, equality rights, increase,their visibility as a social group, build community, and,celebrate sexual diversity and gender variance.,Your,uote.in\n",
            "Cies,ioainane,anein\n",
            "Dl YoU LIKE,BoYs OR GIRLS?,YES.\n",
            "People mayhate you,forbeingdifferentand,bysocietys,living,not,standards, but deep down,,they,they,wish,had the,courageto do the same.,Being,Gay,& Proud,'Qudlese S,geckoandfly.,wwwU.,com\n",
            "KEEP CALM,AND LOVE,WHO YOU WANT,Tar^o,kar\n",
            "T0{%78,ailaoti als ouontS,fo IISPint Pnoce nno rou nnL,fnnno\n",
            "SDUNDS,v,F,IM,IN\n",
            "From the time | was,a kid, Ihave never,been able to,understand attacks upon the gay,community. There are so many,qualities that make up a human,being.,bythe time | get through with all the,things,people,,really,that|,admire about,they,do with their private parts is,what,probably so low on the list that it is irrelevant.,Paul Newman,AciORIDIRECTOR,r\n",
            "Ie,pwcud,oulo,qu ale\n",
            "learned Ihis the hard way...,aoueihersyhow nileelandhck,upoe siynollungand,leo,aluck)vu,uplnstesds\n",
            "IF,FALL,ASLEEP TEXTING,YOU ITS,BECAUSE,DDMT WANT TO,SAY GOODBYE\n",
            "iaesaarnlanis,ineanalnort,imralia,Gan\n",
            "When it comes,to the rights of,LGBT people,the UN is anything,but neutral.,CHARIES RADCLIFFE,UNFREE&LOUAL,alree equal,Gchales_tad,evsnoesosos,\n",
            "When people ask,what Isee in you,,Ijust smile and,look away because,Im afraid ifthey knew,,theyd fall in love,with you too.\n",
            " DAY,s6mTHAVE TO,coMg olT ols,THE CLOlsls2,W L[ J0si3e,WARE INI,AND THATIIL,ALL THAT MAir\n",
            "HAT\n",
            "Whatis straight?A,line can be straight,,ora street, but the,human heart, oh,,no,,it's curved like a,road through,mountains.,Sayinglmages.com,TENNESSEE,WILLIAMS\n",
            "Gey,calture is,surviving and,thrlving. Some,activists believe the,recent rise in,homophobic violence,might be a gauge of,the success of,positive gay images,Lance Loca,Oolefincl.cun\n",
            "When people ask,what Isee in you,,Ijust smile and,look away because,Im afraid ifthey knew,,theyd fall in love,with you too.\n",
            "you,are,LOVABLE,WORTHY,ENOUGH,BRAVE\n",
            "TEEN PROBLEhS\n",
            "Eor,cul5,jir\n",
            "\n",
            "e%8##,#88#8##%#s.\n",
            "l,Inaan:,LGBTQ,NTU,Imaan,LG3Tal,Ms lin,upport,mmian,orguk\n",
            "orr0 ululce,yggy,anesAN,marriages,UEVE,PPOPE STII,PROTESTIN,THIS,SHIT,Simely because iLis noneol,mydamn businesshow,others choose to live their,infos\n",
            "How LGBT Athletes are Claiming their,Play:,Zeigler, author of\"Fair,defied the notion that sports are inherently,Rightful Place in Sports,',homophobic. Part ofwhat is keeping gay athletes closeted, Zeigler said, is,the hyper heterosexism that exists in locker rooms. Negative talk about,is not the issue. Instead, it's the demeaning talk about women.,gay,people\n",
            ":fiosltii,vrnan stiiils uj,mr lrrsilusailiin,kiuswing it jssilly.,illisnt vlainingit.,slirsstiilsuj lir,nll,nooo0,aoinaanou\n",
            "LOVE,KNOWS,SGENDER,NO\n",
            "BITCH,GHETTO,ILLEGAL ALIEN,NO HOMO,RETARDED &LAME,THATS SO GAY,WHOREIHO & SLUT,Wordsfnal HupTzand MHV\n",
            ",,,the UNis anything,but neutral\n",
            "nun,erer,T},Ne6A,ail\n",
            "Qvialoo,DIAUTIFUI,a,ib ulID9vN,ill CIOSIT\n",
            "Jesus,chink,compassionate,,va5,super-intelligent gay man who,understood human problems. On the,cross, he forgave the people who,crucified him. Jesus wanted us to be,loving,and forgiving.,don t knov,what makes people so cruel. Try being,gay woman in the Middle East,as dead,good,s,vou re,Elton John\n",
            "GAY CULTURE IS SURVIVINVG,AND THRIVINVG. SOME,ACTIVISTS BELIEVE THE,RECENT RISEIN,HOMOPHOBIC VIOLENVCE,MIGHT BEA GAUGE OF THE,SuCCESS OF POSITIVE GAY,IMAGES.,uMctlouo,dotenielsgon\n",
            "To be afraid is,to behave as,if the truth were,not true.,BAYARD RUSTIN\n",
            "THIMK,MIRRIIGLISAS,ArIpeIIGChnS,IMIIS BLIN,,BLTMLINAunN,AN0A DnN,GTBT\n",
            "ou dont have to,BE GA,TCBHA,SUPPORTER,iou,just havelto be,Human!,Daniel Radcliffe\n",
            "He are legal!!!,fifty,In all,states!!!,fir{J7\n",
            "mlanratntt,IsNe015,ls3,nav\"Ta0s[[1,TIg[51,1,08>80,e\n",
            "\"BE STRONG,,YOU NEVER,KNOWWHO,YOU ARE,INSPIRING\"\n",
            "mLDOw,tGnQ,st,mlll4\n",
            "Its funnyhowweknowgay,existandwedont,people,ofGodbutwe,have,,proof,deny,gaypeoplebasic,871,human nghts because,itmightpiss offGod.,Being,Gay,& Proud Quotes,geckoandfly.,w7u70,com\n",
            "lesbianvenom:,lesbianvenom:,straighteners aren't worth ur moneyi've been,using,one for three,weeks andi'm still definitelya lesbian,living,in oll my20 years of,this has been byfar mybest joke ever\n",
            "DONT,&,PLEASE,STRAIGHT\n",
            "Back then my,denial wasboth active,and passive.,Caught shadowboxing,mysubconscious,Iwas,determinedto dodge the danger ol,mynatural desires\n",
            "IHUHHPOSTI,3\n",
            "Lile's a game. Playto win\n",
            "MAINOT BE THCSAM,Di TlTSNi MVulM,mRaanV nL MER HuIL,DIVNRIGHTISUPPORTIT,r\n",
            "just heard you say,ihat'seaso,gay!',olhorlilnoe)ou\"@oul6,LGe,Snteresungi,ludicrous,frivolous,icational. bogusstiegi curoys,cunou5,insipla,absurdnidiculous,igooty.,annoying. asinine,Surrea Pathetic.,wuchlyl,yesterday\n",
            "TEENN PROBLEhS\n",
            "THE BEAUTV OF,STANDING UP FOR YOUR,RIGHTSIS OTHERS SEE,YOU STANDING AND,STAND UP AS WELL.,T\n",
            "learned this the hard way..,\"You either sayhow you feel and fuck,it upor say nothing and let it fuck you,up instead.'\n",
            "aoandonna cuinunoo,msm4,a,mn,ananinunna,biteny,donle o0,camDsolnlyo,Inlyl,naanaoura,orgnnbundalgnndno,nomovdon,\n",
            "GLycULTUREIS SURVIVIIICIID,THRIVrc soHcacTvsTs BalAM,TCRECLIT RISLIIHOHOPHOBIC,MOUHHCLMIGHT BCA GLUGLOFT[,SUCCESS OF POSITIVE GIIVIMIGES.,ase5u0\n",
            "LovE,Is To0 BEAunIIL,TO gE HlDDlsl,IN THE CLOSIs\n",
            "Ie uo\n",
            "PROUD,ll8l\n",
            "Notli?3,nill nok,ealess,go,rgla,M-~\n",
            "BE,zsuegece\n",
            "o~,66683,a~s,celceve&,Mese Amelo\n",
            "THERE'S NOTHING,WRONG WITH YOU.,THERE'S A LOT,WRONNG WITH THE,WORLD YOU LIVE IN,C0l[R,CRs,Tano=,kr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TDU86tgJOqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "06892472-9ba0-480f-df68-76f2bf1d58b0"
      },
      "source": [
        "text_data['clean_sentence']=clean_sentences(text_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWdcGpCyJzoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niboHmagPqE3",
        "colab_type": "text"
      },
      "source": [
        "Using Text Blob for sentiment analysis(pretrained model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ITOfQKuWei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qf-zMxBSISQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBGnVN2nmIlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asv8jm-sPp2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e9028c3-4a52-440b-b8ff-7701489eb290"
      },
      "source": [
        "from textblob import TextBlob\n",
        "sent=[]\n",
        "for i in text_data['clean_sentence']:\n",
        "  print(i)\n",
        "  t=TextBlob(i)\n",
        "  print(t.sentiment)\n",
        "  sent.append(t.sentiment.polarity)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loses  slo \n",
            "Sentiment(polarity=-0.3, subjectivity=0.1)\n",
            "married uith pride\n",
            "Sentiment(polarity=0.25, subjectivity=0.25)\n",
            "mretahen drnial leinactir ondvaoowz caughi shadovshoning miycubronerioudlwao feternineaio dolzainadiero naturafasine\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "being gayis nota crime anditis nota sin  stop using godto justifyyour prejudice  religion is aboutlovingone another you rejust looking foran excuse tohate  being gay   proud quotes geckoandfly  wwu u com\n",
            "Sentiment(polarity=0.3888888888888889, subjectivity=0.5444444444444444)\n",
            "hat\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "saluinaovvoring ar lalentzd zyen nere qain noubl iibian tosgnanol  oucan cfchins  lalding n  moll gno\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "ts rusan unsesla   a   noe amolly blkelea byune seatus quo is repulsve\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "gt\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "be ilurshe naoriyinaly alwu wonl ore thu  co   \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "for lgbt\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "  bestgay quotes www gd bl  g ne t\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "google beovor naans booontaaa  nornosanunny boasenseaa  aborlon bor armartgg  t \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "uguu ci a  mit  snoutonuingcytntda datgngr um nlnu st jntiutngn culdg  nru nyegoncgrnseci mennnso urnnrnu guntag cilnuucurrnam\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "if haryy poller pught us anylhing it s that no one should live in a closet\n",
            "Sentiment(polarity=0.13636363636363635, subjectivity=0.5)\n",
            "hate ir has causedalot or tnoullusn this wohld  huti  has aotsolved oe vet may  avgelou\n",
            "Sentiment(polarity=-0.8, subjectivity=0.9)\n",
            "love is too beautiil to be hiddein in the closet\n",
            "Sentiment(polarity=0.5, subjectivity=0.6)\n",
            "dl stgay quotes \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "locsy toc\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "wo ms scanl gelect  me  ne mest t or be eelectee mase agela\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "burst dovn those closet doors once and for all  and stand up and start to fight  harvey mill gsnalmnaci corn\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "do yu realize whar you when you say sa that s so gayl   gay  hor mrcn cno  no   tobo\n",
            "Sentiment(polarity=0.4166666666666667, subjectivity=0.5833333333333334)\n",
            "you oppose gay rights so because of the bible  try mnless you also to outlaw shrime cocktails ilev              cursing il ev deut       women s jeans       arrogance iprov   lying iprov         bacon ilev       aclultery deut         working sunday inum       i on please shut the hell upl\n",
            "Sentiment(polarity=0.4166666666666667, subjectivity=0.5833333333333334)\n",
            "oooo verizon lte       am     x tweet      anything negative apologize for r ive said towards gays and for that matter anyone    was young  ignorant  stupid  and in a bad place  i ve moved on and learned from my mistakes and am so truly sorry to anyone have have nothing against offended  anything promotes that anyone or sorry equality  again w e r t p k s h d f j a z x b n m         space\n",
            "Sentiment(polarity=-0.44999999999999996, subjectivity=0.7444444444444445)\n",
            "t\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "diamirer rlrarnisorie mra   \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "   gvally  otve   pnie   \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "     qouidel drcamstimc com                barrirret\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "the biblical case in support of same sex relationships   and g c  the a  y christian matthew vines\n",
            "Sentiment(polarity=0.0, subjectivity=0.0625)\n",
            "hate it has caused a lot of problems in this world  buit is has not solved one yet  maya angelou\n",
            "Sentiment(polarity=-0.8, subjectivity=0.9)\n",
            "ppeople dont know ifim gay straight or an alien from outer space   its funny  gerard wag\n",
            "Sentiment(polarity=0.15416666666666667, subjectivity=0.6833333333333333)\n",
            "sdunds giy im in\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "zol       nhht  ay ste thela gw\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "keep calm and love who you want\n",
            "Sentiment(polarity=0.4, subjectivity=0.675)\n",
            "ithinka    marriage is as a marriage has alwavs been    betweena man and a woman  mluey pdomamcliivtdi   db  h tbt\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "turaeayrerou ram anrngnnlor mnnr aalern irn oarlanlmnnr ce\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "memll notbe ess  y eraed\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "irr zlrlr authol  lr mliy  lln luot nihl   cing duilengiailar inlensuly riehinltt        ranolwhatfbovtunn                      mnsralisvtalkalnat     instjsildlnnin rllal ml\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "being gayis like being lefthanded  some people  are most people arent and nobodyreallyknows why  its notrightorwong  itsjustthe waythings are  being gay   proud quotes geckoandfly  wwu u com\n",
            "Sentiment(polarity=0.5722222222222223, subjectivity=0.6944444444444445)\n",
            "cny cudiunhis suavivio ncivi son ig ioxnce hom  qulc succis  gn cnude  ofrosiiiyl mndg  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "mocte goy dosce ng   vomlne ool evel oos eo mzveir annrsa ord   urego\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "  lov  is aeve   \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "the pressures on gay teens can be overwhelming to keep secrets  tell lies  deny who are  and try you to be who you re not  remember  you are special and worth being cared about  loved  and accepted just as you are  never  ever let anyone convince you otherwise aler sancher aulhor\n",
            "Sentiment(polarity=0.45476190476190476, subjectivity=0.6109523809523811)\n",
            "if i could have chosen to be gay or straight  i thinki would have simply chosen to be happy   k hlrp\n",
            "Sentiment(polarity=0.3541666666666667, subjectivity=0.5851190476190476)\n",
            "eo msslest cartgo p tm becalse caat  o me  mase anelo\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "i\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "wdodud od da wais benis tn vs mat  pe dlade lnis iyjy  dr gkc sl   len t mpte icto ml gm in ay\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "iffncouulp have cmosen to be    naversmmply chosen to be happy\n",
            "Sentiment(polarity=0.8, subjectivity=1.0)\n",
            "qwialso bauiifui a fotrrbbn it ciost\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "  tragedy in their some lives  but tve onlyhad tragic haircuts and\n",
            "Sentiment(polarity=-0.75, subjectivity=0.75)\n",
            "life s a game  play to win \n",
            "Sentiment(polarity=0.2, subjectivity=0.4)\n",
            "vality   avve  pile \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "jason j   jaybs       added me everyone add my new snapchat or comment names below  iii be posting positive quotes every day igbtpride loveislove igbt trans transmatter transgender transgirl transmale lesbian lesbiansex gaypride gay bi bisexualpride bisexual bipride asexual asexualpride pansexualpride pansexual protecttranskids swag ftm pride mtf\n",
            "Sentiment(polarity=0.2601010101010101, subjectivity=0.5277777777777778)\n",
            "nunal aaulaen t lue fostananuuusnunaieuu t majny lslesti  p nraal tne tnut semimanay irun novsoaunihjs alv slsin tuntesuofslav  tees ls alajelrile negouse tanray  e alu u  itelnlt  mnneinan fuuca nse  acie aiun lalu fdusunn mz sstknssetiestile t rsi ori  ausu fevurate natray lln srneval ik ikefrol drnvalnls  uutreosoy seaelae nr  atilyauoue  uin nasaulalsulat avuno reanrn tahaltnvanlurz unugsva lsutas tlsytusnov luutleni nannulh luu ee     ale oylolrirn nan  i   n   m suenidsme retersoilgsure fuuayalotunnfsrnmun m a s fuautun ncdculnn nossiell ne usaauldalea olue  b  laeuo fingsin rlsuear rs   annteigunrne fouasal inarruraie ntraallnlannet regak e      ioknso natnn r tyika nsstinlskal ebunemanonesnsllnoheleikusulte  na  wion wa inuylds   tutalr talsjasmlt steueyael utugi lunir  n    tr  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "so you oppose say tigits because of the bible  to outlaw try unless you also i   cooikal il        nvy   irrgs   tero  tisli nceo acer  idvu  lji merrn       a please shut the hell u \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "ithink being gavisa blessing   anditssomething iamthanful for evervsinglediv d rs  nr c  p   t \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "onne day we wont have to icome out of the closet  well just say we are in lov  and that will all that matt\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "if harty poller pughl us onylhing irsunalnojois aonlalislm ajdanart\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "be yurshe naorinincly al  wotk more thar a cop  \n",
            "Sentiment(polarity=0.5, subjectivity=0.5)\n",
            "ge find oul who you are land ba thal p arson  thal s anal your soul was put on unis earuh to bs ind tnal trulh tve tnat uruln and evannning slse unil core   \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "if harty poller onylhing pught us irreuhal no onie shoula liin a dosil\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "accepance of yourself isearmore important than acceptance inom orhers \n",
            "Sentiment(polarity=0.4, subjectivity=1.0)\n",
            "cci you can you re halfivay z  r  kaasul \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "keep calm becauseit s okay to be gay\n",
            "Sentiment(polarity=0.4055555555555556, subjectivity=0.6111111111111112)\n",
            "i  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "yov ullot havl rolnut tit hrd tenilve fifd \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "             tralh \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "gg    top pro   con quotes\n",
            "Sentiment(polarity=0.5, subjectivity=0.5)\n",
            "l am nol liece while anywoman is unliee  even when her shackles are very dillerent liom my ownl  audre lorde gh\n",
            "Sentiment(polarity=0.2, subjectivity=0.3)\n",
            "do you realize what you say  when you say ihat s so gay    the way has ddays is usednowa  gay a negative connotation  which it should not  gay means brighily colored happy and or homosoxual hor ammoyimg  srupid  foolish  patheiic  auplenous rnailouneunlous     weak  docus  nidiculous  fnivolous  hildish  nusund awkwand  on fneaky choosea wqrd wich means what you mganio say  iis only ihe smart ihing io do \n",
            "Sentiment(polarity=0.12053571428571429, subjectivity=0.677232142857143)\n",
            "you say  be stfight  i say taste the rainbow\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "ln dfrai  nof of l uio e\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "iamgaf iamstraghe  iameeshian iamhisexuae  iamfransgendered  iam human \n",
            "Sentiment(polarity=0.0, subjectivity=0.1)\n",
            "ie m  ess a to w nores will kn nver m la amei  can le zou m  anela\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "we are powerful because we have survived  audre lorde\n",
            "Sentiment(polarity=0.3, subjectivity=1.0)\n",
            "be ha\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            " shine yur soulwith the same humility egoless as the rainbow andno matter where you go in this worddorthe nert  love wilfindyou  attendyou audbless you aberhani grom jouneythrughihe pouerofihe ranbou kabvu  an bysara l\n",
            "Sentiment(polarity=0.25, subjectivity=0.3625)\n",
            "tatergreram dnallaar lrn nanllya fnn\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "being gayis nota crime anditis nota sin  stop using godto justifyyour prejudice  religion is aboutlovingone another you rejust looking foran excuse tohate  being gay   proud quotes geckoandfly  wwu u com\n",
            "Sentiment(polarity=0.3888888888888889, subjectivity=0.5444444444444444)\n",
            "b becter togtl gts\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            " onstage  devil  but im hardlya lam a social reject  hercarg freddle naelamaa ian aranaleno\n",
            "Sentiment(polarity=0.03333333333333333, subjectivity=0.06666666666666667)\n",
            "addidle\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "cleliene you can   you  re halfway zee  t  roosevelt\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            " even in the bible we canread na ang babae  dapat magsuot na pambabae at ang lalaki  magsuot ng panlalaki  thats whati believe \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "go there s nothing wrong wirh you  theres a lof wrong wirh the world you live in cirs coller\n",
            "Sentiment(polarity=-0.2878787878787879, subjectivity=0.7666666666666666)\n",
            "gc happiness is whenwhat youthink  whatyou say and what you doarein harmony    inopungandpouavequolca com inspiring and positive quotes   \n",
            "Sentiment(polarity=0.47575757575757577, subjectivity=0.5818181818181818)\n",
            "nin nint lrii hilsirsv ini muolru t        vliru lir sl ll   ariyrrylillirinl iriiiniyii  nueoane e \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "nothinc would pe the spme e  oud d notexist\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "myamrnro ioanlosurlnsndl  seren no coalnaa  a  calo     s ol \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "marriage  s about loy notcender\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "being gayis like being lefthanded  some people  are most people arent and nobodyreallyknows why  its notrightorwong  itsjustthe waythings are  being gay   proud quotes geckoandfly  wwu u com\n",
            "Sentiment(polarity=0.5722222222222223, subjectivity=0.6944444444444445)\n",
            "godand gay the christian mattheiv vies\n",
            "Sentiment(polarity=0.20833333333333334, subjectivity=0.2916666666666667)\n",
            "gn\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "anti lgbt  states that have laws restricting teachers and stafffrom talking about lgbt issues at school alabima ailona toulstana mlssissippl ouanoma soulh carollna tna  urah\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "anti bullying  states that have laws prohibiting bullying of students on the bases ofsexual orientation and gender identity ailansas calllornla colorido connedticut mlllnols lon miln maryland massacnusetts minnesot  mevida men mampsnlre menlerssy msn lorl norih carolina oregon phodelsland vermont washington washington  dc\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "love gve\n",
            "Sentiment(polarity=0.5, subjectivity=0.6)\n",
            "jefeeleern  aa  maureu lse myusng tael f ram  liraye mol       funanne tazs alueeuna ae ngala eenelunue l           dalna mnuae rsp lalneil   e    ane  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "we ore powertul becouse we hove sur vived r\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "m    r \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "alotofparents will do anythingfor their kids exceptletthem be themselves  being gay   proud quotes o geckoandfly  com ww u\n",
            "Sentiment(polarity=0.6083333333333334, subjectivity=0.7916666666666667)\n",
            "lo belolroid ls co behove os che erurh were noe crue \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "korned composoion lrom bcing discrminotcd ogoinst  everyening bod uhovs hopoeneo to hos couohe me comoossons \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            " ozel e o  c       auae aelo\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "you re not out and proud  yours urte outannabestmontn ine don spendanolner hiding closet andsnamne ou donina for roudeserv and your trib naitingto walcomo vouhome\n",
            "Sentiment(polarity=0.8, subjectivity=1.0)\n",
            "claimingthat someone else s marniage is against being your religion islike angrywth someone for eatinga donutwhen you re on a diet  being gay   proud quotes geckoandfly  wwwu  com\n",
            "Sentiment(polarity=0.6083333333333334, subjectivity=0.7916666666666667)\n",
            "ofcourse gaymen dress well  theydidnt spend all thattime in the closet doing nothing  being gay   proud quotes www geckoandfly com\n",
            "Sentiment(polarity=0.6083333333333334, subjectivity=0.7916666666666667)\n",
            "sounds ga  im in ee\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "igoieaisofiiliiiigstorics\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "cc find out who you are and be that person  that s what your soul on this earth was put to be  find that truth  live that truth  and everything else will come  ellen   \n",
            "Sentiment(polarity=0.13636363636363635, subjectivity=0.5)\n",
            "iwish more teachers could elaborate on it ilgbtq topics  and talk about it more  instead of like  two sentences and then dismiss the subject  elaina in wnat do you know  sulo twelve year olds tal abouttgaro topics welcoming schools film \n",
            "Sentiment(polarity=0.3333333333333333, subjectivity=0.5833333333333334)\n",
            "cru  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "learned compassion from being discriminated against  everything bad that s ever happened to me has taught compassion  me degeneres ellen\n",
            "Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)\n",
            "    o   ncalaiaie lanna iriserda m  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "r  jurnu  re\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "we are legal   hilty inall states \n",
            "Sentiment(polarity=0.2, subjectivity=0.2)\n",
            "islongjss uno esrerlabon cinoln nastales stonen beleg belemens sumi as ennal fuur ang suna bin ula nol nddlnbronesaalsydors aa  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "gay pride or lgbt pride is the positive stance against discrimination and violence toward lesbian  gay  bisexual  and transgender  lgbt  people to promote their self affirmation  dignity  equality rights  increase their visibility as a social group  build community  and celebrate sexual diversity and gender variance  your uote in\n",
            "Sentiment(polarity=0.3187878787878788, subjectivity=0.5224242424242425)\n",
            "cies ioainane anein\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "dl you like boys or girls  yes \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "people mayhate you forbeingdifferentand bysocietys living not standards  but deep down  they they wish had the courageto do the same  being gay   proud  qudlese s geckoandfly  wwwu  com\n",
            "Sentiment(polarity=0.21222222222222223, subjectivity=0.47944444444444445)\n",
            "keep calm and love who you want tar o kar\n",
            "Sentiment(polarity=0.4, subjectivity=0.675)\n",
            "t      ailaoti als ouonts fo iispint pnoce nno rou nnl fnnno\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "sdunds v f im in\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "from the time   was a kid  ihave never been able to understand attacks upon the gay community  there are so many qualities that make up a human being  bythe time   get through with all the things people  really that  admire about they do with their private parts is what probably so low on the list that it is irrelevant  paul newman acioridirector r\n",
            "Sentiment(polarity=0.13958333333333334, subjectivity=0.4604166666666667)\n",
            "ie pwcud oulo qu ale\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "learned ihis the hard way    aoueihersyhow nileelandhck upoe siynollungand leo aluck vu uplnstesds\n",
            "Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666)\n",
            "if fall asleep texting you its because ddmt want to say goodbye\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "iaesaarnlanis ineanalnort imralia gan\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "when it comes to the rights of lgbt people the un is anything but neutral  charies radcliffe unfree loual alree equal gchales tad evsnoesosos \n",
            "Sentiment(polarity=0.0, subjectivity=0.25)\n",
            "when people ask what isee in you  ijust smile and look away because im afraid ifthey knew  theyd fall in love with you too \n",
            "Sentiment(polarity=0.06666666666666667, subjectivity=0.5333333333333333)\n",
            "day s mthave to comg olt ols the clolsls  w l  j si e ware ini and thatiil all that mair\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "hat\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "whatis straight a line can be straight  ora street  but the human heart  oh  no  it s curved like a road through mountains  sayinglmages com tennessee williams\n",
            "Sentiment(polarity=0.13333333333333333, subjectivity=0.3)\n",
            "gey calture is surviving and thrlving  some activists believe the recent rise in homophobic violence might be a gauge of the success of positive gay images lance loca oolefincl cun\n",
            "Sentiment(polarity=0.23598484848484846, subjectivity=0.3446969696969697)\n",
            "when people ask what isee in you  ijust smile and look away because im afraid ifthey knew  theyd fall in love with you too \n",
            "Sentiment(polarity=0.06666666666666667, subjectivity=0.5333333333333333)\n",
            "you are lovable worthy enough brave\n",
            "Sentiment(polarity=0.4083333333333333, subjectivity=0.75)\n",
            "teen problehs\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "eor cul  jir\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "e              s \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "l inaan  lgbtq ntu imaan lg tal ms lin upport mmian orguk\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "orr  ululce yggy anesan marriages ueve ppope stii protestin this shit simely because ilis noneol mydamn businesshow others choose to live their infos\n",
            "Sentiment(polarity=-0.03181818181818183, subjectivity=0.65)\n",
            "how lgbt athletes are claiming their play  zeigler  author of fair defied the notion that sports are inherently rightful place in sports   homophobic  part ofwhat is keeping gay athletes closeted  zeigler said  is the hyper heterosexism that exists in locker rooms  negative talk about is not the issue  instead  it s the demeaning talk about women  gay people\n",
            "Sentiment(polarity=0.30833333333333335, subjectivity=0.6166666666666667)\n",
            " fiosltii vrnan stiiils uj mr lrrsilusailiin kiuswing it jssilly  illisnt vlainingit  slirsstiilsuj lir nll nooo  aoinaanou\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "love knows sgender no\n",
            "Sentiment(polarity=0.5, subjectivity=0.6)\n",
            "bitch ghetto illegal alien no homo retarded  lame thats so gay whoreiho   slut wordsfnal huptzand mhv\n",
            "Sentiment(polarity=-0.3266666666666666, subjectivity=0.6766666666666666)\n",
            "   the unis anything but neutral\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "nun erer t  ne a ail\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "qvialoo diautifui a ib ulid vn ill ciosit\n",
            "Sentiment(polarity=-0.5, subjectivity=1.0)\n",
            "jesus chink compassionate  va  super intelligent gay man who understood human problems  on the cross  he forgave the people who crucified him  jesus wanted us to be loving and forgiving  don t knov what makes people so cruel  try being gay woman in the middle east as dead good s vou re elton john\n",
            "Sentiment(polarity=0.18787878787878787, subjectivity=0.5257575757575758)\n",
            "gay culture is survivinvg and thrivinvg  some activists believe the recent risein homophobic violenvce might bea gauge of the success of positive gay images  umctlouo dotenielsgon\n",
            "Sentiment(polarity=0.2721212121212121, subjectivity=0.39242424242424245)\n",
            "to be afraid is to behave as if the truth were not true  bayard rustin\n",
            "Sentiment(polarity=-0.38749999999999996, subjectivity=0.775)\n",
            "thimk mirriiglisas aripeiigchns imiis blin  bltmlinaunn an a dnn gtbt\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "ou dont have to be ga tcbha supporter iou just havelto be human  daniel radcliffe\n",
            "Sentiment(polarity=0.0, subjectivity=0.1)\n",
            "he are legal    fifty in all states    fir j \n",
            "Sentiment(polarity=0.2, subjectivity=0.2)\n",
            "mlanratntt isne    ls  nav ta s    tig            e\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            " be strong  you never knowwho you are inspiring \n",
            "Sentiment(polarity=0.4666666666666667, subjectivity=0.8666666666666667)\n",
            "mldow tgnq st mlll \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "its funnyhowweknowgay existandwedont people ofgodbutwe have  proof deny gaypeoplebasic     human nghts because itmightpiss offgod  being gay   proud quotes geckoandfly  w u   com\n",
            "Sentiment(polarity=0.4055555555555556, subjectivity=0.5611111111111111)\n",
            "lesbianvenom  lesbianvenom  straighteners aren t worth ur moneyi ve been using one for three weeks andi m still definitelya lesbian living in oll my   years of this has been byfar mybest joke ever\n",
            "Sentiment(polarity=0.3, subjectivity=0.1)\n",
            "dont   please straight\n",
            "Sentiment(polarity=0.2, subjectivity=0.4)\n",
            "back then my denial wasboth active and passive  caught shadowboxing mysubconscious iwas determinedto dodge the danger ol mynatural desires\n",
            "Sentiment(polarity=-0.06666666666666667, subjectivity=0.3)\n",
            "ihuhhposti  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "lile s a game  playto win\n",
            "Sentiment(polarity=0.2, subjectivity=0.4)\n",
            "mainot be thcsam di tltsni mvulm mraanv nl mer huil divnrightisupportit r\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "just heard you say ihat seaso gay   olhorlilnoe ou  oul  lge snteresungi ludicrous frivolous icational  bogusstiegi curoys cunou  insipla absurdnidiculous igooty  annoying  asinine surrea pathetic  wuchlyl yesterday\n",
            "Sentiment(polarity=-0.4611111111111111, subjectivity=0.8277777777777778)\n",
            "teenn problehs\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "the beautv of standing up for your rightsis others see you standing and stand up as well  t\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "learned this the hard way    you either sayhow you feel and fuck it upor say nothing and let it fuck you up instead  \n",
            "Sentiment(polarity=-0.36388888888888893, subjectivity=0.5805555555555556)\n",
            "aoandonna cuinunoo msm  a mn ananinunna biteny donle o  camdsolnlyo inlyl naanaoura orgnnbundalgnndno nomovdon \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "glycultureis surviviiiciid thrivrc sohcactvsts balam tcreclit risliihohophobic mouhhclmight bca glugloft  success of positive giivimiges  ase u \n",
            "Sentiment(polarity=0.2636363636363636, subjectivity=0.2727272727272727)\n",
            "love is to  beauniil to ge hlddlsl in the closis\n",
            "Sentiment(polarity=0.5, subjectivity=0.6)\n",
            "ie uo\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "proud ll l\n",
            "Sentiment(polarity=0.8, subjectivity=1.0)\n",
            "notli   nill nok ealess go rgla m  \n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "be zsuegece\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "o        a s celceve  mese amelo\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "there s nothing wrong with you  there s a lot wronng with the world you live in c l r crs tano  kr\n",
            "Sentiment(polarity=-0.18181818181818182, subjectivity=0.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ySjmbNPpjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "3e505956-ba66-4f43-9abe-6727f2ff1138"
      },
      "source": [
        "# text_data.insert(4,'Category',sent,True)\n",
        "text_data['Category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0.000000    106\n",
              " 0.200000      6\n",
              " 0.500000      6\n",
              " 0.608333      3\n",
              " 0.400000      3\n",
              " 0.800000      3\n",
              " 0.136364      2\n",
              " 0.405556      2\n",
              " 0.416667      2\n",
              " 0.572222      2\n",
              "-0.800000      2\n",
              " 0.388889      2\n",
              " 0.066667      2\n",
              " 0.250000      2\n",
              " 0.300000      2\n",
              " 0.033333      1\n",
              " 0.408333      1\n",
              " 0.208333      1\n",
              "-0.700000      1\n",
              " 0.308333      1\n",
              "-0.287879      1\n",
              " 0.354167      1\n",
              " 0.466667      1\n",
              " 0.475758      1\n",
              " 0.454762      1\n",
              " 0.120536      1\n",
              "-0.500000      1\n",
              " 0.212222      1\n",
              "-0.750000      1\n",
              " 0.318788      1\n",
              "-0.326667      1\n",
              " 0.139583      1\n",
              "-0.363889      1\n",
              "-0.300000      1\n",
              " 0.187879      1\n",
              "-0.291667      1\n",
              "-0.461111      1\n",
              " 0.260101      1\n",
              " 0.235985      1\n",
              "-0.387500      1\n",
              " 0.272121      1\n",
              " 0.154167      1\n",
              " 0.263636      1\n",
              "-0.031818      1\n",
              "-0.450000      1\n",
              "-0.066667      1\n",
              "-0.181818      1\n",
              " 0.133333      1\n",
              " 0.333333      1\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWJr0JY1LfE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPZdp8bgLVPN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3d184927-a703-478a-b9b2-59bb73ee540a"
      },
      "source": [
        "\"\"\"\n",
        "import flair\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "pred=[]\n",
        "for i in text_data['clean_text_for_flair']:\n",
        "  s = flair.data.Sentence(i)\n",
        "  flair_sentiment.predict(s)\n",
        "  total_sentiment = s.labels\n",
        "  pred.append(total_sentiment)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport flair\\nflair_sentiment = flair.models.TextClassifier.load('en-sentiment')\\npred=[]\\nfor i in text_data['clean_text_for_flair']:\\n  s = flair.data.Sentence(i)\\n  flair_sentiment.predict(s)\\n  total_sentiment = s.labels\\n  pred.append(total_sentiment)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27n7n0bKML8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_data.insert(5,'Category_by_flair',pred,True)\n",
        "# text_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pabY8M10PiT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_data1=text_data[:] # copy the data for safety\n",
        "# text_data1=text_data1.drop(columns='Category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY4TUgDFNK_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "910b3cb0-956f-46b9-b080-5355e1645cb5"
      },
      "source": [
        "text_data1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(164, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AysIe9ZSR1rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2a7851d0-3cc2-43f7-ee8c-fb244c13b095"
      },
      "source": [
        "random_data['Category']='Random'\n",
        "random_data=random_data.loc[:,['filename','Category']]\n",
        "random_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test128.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test240.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Test160.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Test173.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Test212.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      filename Category\n",
              "0  Test128.jpg   Random\n",
              "1  Test240.jpg   Random\n",
              "5  Test160.jpg   Random\n",
              "8  Test173.jpg   Random\n",
              "9  Test212.jpg   Random"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3CBmRIYXyrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "586a4e48-a151-4d02-9178-e92d544613e7"
      },
      "source": [
        "# test=test.rename({'sentiment':'Category'},axis=1)\n",
        "# test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test181.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test216.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test979.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Test1634.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Test144.jpg</td>\n",
              "      <td>Random</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        filename Category\n",
              "0    Test181.jpg   Random\n",
              "2    Test216.jpg   Random\n",
              "4    Test979.jpg   Random\n",
              "8   Test1634.jpg   Random\n",
              "10   Test144.jpg   Random"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuwWUujHh6xD",
        "colab_type": "text"
      },
      "source": [
        "merging dataframe to find the final one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru9L_0z42wXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ca21ab37-dc8b-4986-d571-b4ed0d2a7f68"
      },
      "source": [
        "# train2=text_data.loc[:,['filename','Category']]\n",
        "train2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test751.jpg</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test100.jpg</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test533.jpg</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test3827.jpg</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test211.jpg</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       filename Classification\n",
              "0   Test751.jpg       Negative\n",
              "1   Test100.jpg        Neutral\n",
              "2   Test533.jpg       Positive\n",
              "3  Test3827.jpg       Negative\n",
              "4   Test211.jpg       Negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUEEoeeA53IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train2=train2.replace({'Classification':'Category'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X55K0nps2wMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4249834e-13dc-4e44-d814-801b17820244"
      },
      "source": [
        "# train2['Category']=train2['Category'].str.replace('4','Positive')\n",
        "# train2['Category']=train2['Category'].str.replace('0','Negative')\n",
        "train2=train2.replace()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive    118\n",
              "Negative     46\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H3CW-fyYI-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2=pd.concat([train2,random_data])\n",
        "# df2=df2.drop(columns=['text','length'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFVONJ-V4z4H",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "266fd57f-5207-4167-95fe-785560a079f4"
      },
      "source": [
        "from google.colab import files\n",
        "f=files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b555b993-f3a7-4d96-9650-5e82065407d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b555b993-f3a7-4d96-9650-5e82065407d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Test.csv to Test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuEZ8-LZYXCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a24a42a1-6042-4431-e1a6-e7b54be856ed"
      },
      "source": [
        "df1=pd.read_csv('Test.csv')\n",
        "df1=df1.drop(columns='Category')\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test1001.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test1012.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test1022.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test1071.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test1122.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Filename\n",
              "0  Test1001.jpg\n",
              "1  Test1012.jpg\n",
              "2  Test1022.jpg\n",
              "3  Test1071.jpg\n",
              "4  Test1122.jpg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Ns2F-4YpHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1ea6d5d0-d18a-4b95-b206-6f05537d9ba6"
      },
      "source": [
        "final=pd.merge(df1,df2,left_on='Filename',right_on='filename',how='left')\n",
        "final=final.drop(columns='filename')\n",
        "# final=final.reset_index(drop=True,inplace=True)\n",
        "final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Classification</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test1001.jpg</td>\n",
              "      <td>Positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test1012.jpg</td>\n",
              "      <td>Positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test1022.jpg</td>\n",
              "      <td>Positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test1071.jpg</td>\n",
              "      <td>Positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test1122.jpg</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Filename Classification Category\n",
              "0  Test1001.jpg       Positive      NaN\n",
              "1  Test1012.jpg       Positive      NaN\n",
              "2  Test1022.jpg       Positive      NaN\n",
              "3  Test1071.jpg       Positive      NaN\n",
              "4  Test1122.jpg        Neutral      NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XjQmTwqZjvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final.to_csv('1.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD5E8ST7Zxl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "afea5241-9bb1-4f77-a484-793b225dc2cf"
      },
      "source": [
        "from google.colab import files\n",
        "f=files.download('1.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_42449906-e170-4901-b8cb-b012eb29f573\", \"1.csv\", 5996)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ZgdVlMP6fE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ab3b2983-36d8-45db-c979-f4e518ce3146"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu6dtZ5d5QFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5e677498-3e07-4367-a477-681985d43ae7"
      },
      "source": [
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def clean_sentences1(df):\n",
        "    reviews = []\n",
        "\n",
        "    for sent in (df['clean_sentences']):\n",
        "        \n",
        "        #remove html content\n",
        "        review_text = BeautifulSoup(sent).get_text()\n",
        "        \n",
        "        #remove non-alphabetic characters\n",
        "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "        #tokenize the sentences\n",
        "        words = word_tokenize(review_text.lower())\n",
        "        \n",
        "        stops = set(stopwords.words(\"english\"))                  \n",
        "    # \n",
        "        # 5. Remove stop words\n",
        "        meaningful_words = [w for w in words if not w in stops]\n",
        "    \n",
        "        #lemmatize each word to its lemma\n",
        "        lemma_words = [lemmatizer.lemmatize(i) for i in meaningful_words]\n",
        "    \n",
        "        reviews.append(lemma_words)\n",
        "\n",
        "    return(reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SABAtsSW-4Pq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "cbfc4ed7-4f1e-45b2-f8c2-5b3145fe9657"
      },
      "source": [
        "dataset['preprocessed_text']=clean_sentences(text_data)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test181.jpg</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test119.jpg</td>\n",
              "      <td>@eeeo Verizon LTE 10:47 AM —@%3%eD\\n\\n| apolog...</td>\n",
              "      <td>[eeeo, verizon, lte, ed, apologize, anything, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test216.jpg</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test937.jpg</td>\n",
              "      <td>you are\\n¥ LOVABLE\\n\\nFY WORTHY\\nFY ENOUGH\\nWw...</td>\n",
              "      <td>[lovable, fy, worthy, fy, enough, ww, brave]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test979.jpg</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      filename  ...                                  preprocessed_text\n",
              "0  Test181.jpg  ...                                                 []\n",
              "1  Test119.jpg  ...  [eeeo, verizon, lte, ed, apologize, anything, ...\n",
              "2  Test216.jpg  ...                                                 []\n",
              "3  Test937.jpg  ...       [lovable, fy, worthy, fy, enough, ww, brave]\n",
              "4  Test979.jpg  ...                                                 []\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHyayZMSUCvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01816ce5-cbcb-4c9f-fc1a-a39700b402e8"
      },
      "source": [
        "unique_words = set()\n",
        "len_max = 0\n",
        "\n",
        "for sent in (text_data['clean_sentence']):\n",
        "    \n",
        "    unique_words.update(sent)\n",
        "    \n",
        "    if(len_max<len(sent)):\n",
        "        len_max = len(sent)\n",
        "        \n",
        "#length of the list of unique_words gives the no of unique words\n",
        "print(len(list(unique_words)))\n",
        "print(len_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1371\n",
            "160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je6ukvgNtxLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_X = tokenizer.texts_to_sequences(text_data['clean_sentence'])\n",
        "\n",
        "train_data_X = sequence.pad_sequences(train_data_X,maxlen=len_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7-0lTOTpCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "58ea49e3-d185-42a2-b2a4-72ad77e696df"
      },
      "source": [
        "print(train_data_X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[     0      0      0 ...  17597  17143  24242]\n",
            " [     0      0      0 ...   1861 412274   7471]\n",
            " [     0      0      0 ...      0  35851   7817]\n",
            " ...\n",
            " [     0      0      0 ...   5554  21578  14010]\n",
            " [     0      0      0 ...      0      0      0]\n",
            " [     0      0      0 ...  63632  16507  65180]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Buy_-D8XwDQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1d88cbac-435e-4967-a625-44119960db40"
      },
      "source": [
        "# train_data['sentiment']=pred\n",
        "# train_data.head()\n",
        "train_data1=train_data[:]\n",
        "train_data1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>length</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test660.jpg</td>\n",
              "      <td>Se aad\\n\\nate\\nCB LSU\\neer ee a\\nSenn\\n\\n2g) M...</td>\n",
              "      <td>[se, aad, ate, cb, lsu, eer, ee, senn, g, hy, ...</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test526.jpg</td>\n",
              "      <td>hLedemhenia\\nLeslee teriebenel\\nPe arcleneteal...</td>\n",
              "      <td>[hledemhenia, leslee, teriebenel, pe, arclenet...</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Test523.jpg</td>\n",
              "      <td>Cle) Dy\\nAi GAY\\n\\nera Swan</td>\n",
              "      <td>[cle, dy, ai, gay, era, swan]</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Test1071.jpg</td>\n",
              "      <td>ees MU WALLA OL\\n\\nem eae ie\\n\\nnot living by ...</td>\n",
              "      <td>[ee, mu, walla, ol, em, eae, ie, living, socie...</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Test668.jpg</td>\n",
              "      <td>@\\n\\nX/hen it come:\\nto the rights of\\n\\nthe U...</td>\n",
              "      <td>[x, hen, come, right, un, anything, neutral, em]</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        filename  ... sentiment\n",
              "1    Test660.jpg  ...         0\n",
              "2    Test526.jpg  ...         0\n",
              "6    Test523.jpg  ...         0\n",
              "8   Test1071.jpg  ...         4\n",
              "11   Test668.jpg  ...         0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKqRP5hVyZ-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e1c9c75b-d1b4-45e5-8bfb-f7dc810a87af"
      },
      "source": [
        "train_data1['sentiment']=train_data1['sentiment'].replace('negative','Negative')\n",
        "train_data1['sentiment']=train_data1['sentiment'].replace('positive','Positive')\n",
        "train_data1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>length</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test660.jpg</td>\n",
              "      <td>Se aad\\n\\nate\\nCB LSU\\neer ee a\\nSenn\\n\\n2g) M...</td>\n",
              "      <td>[se, aad, ate, cb, lsu, eer, ee, senn, g, hy, ...</td>\n",
              "      <td>14</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test526.jpg</td>\n",
              "      <td>hLedemhenia\\nLeslee teriebenel\\nPe arcleneteal...</td>\n",
              "      <td>[hledemhenia, leslee, teriebenel, pe, arclenet...</td>\n",
              "      <td>12</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Test523.jpg</td>\n",
              "      <td>Cle) Dy\\nAi GAY\\n\\nera Swan</td>\n",
              "      <td>[cle, dy, ai, gay, era, swan]</td>\n",
              "      <td>6</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Test1071.jpg</td>\n",
              "      <td>ees MU WALLA OL\\n\\nem eae ie\\n\\nnot living by ...</td>\n",
              "      <td>[ee, mu, walla, ol, em, eae, ie, living, socie...</td>\n",
              "      <td>20</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Test668.jpg</td>\n",
              "      <td>@\\n\\nX/hen it come:\\nto the rights of\\n\\nthe U...</td>\n",
              "      <td>[x, hen, come, right, un, anything, neutral, em]</td>\n",
              "      <td>8</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        filename  ... sentiment\n",
              "1    Test660.jpg  ...  Negative\n",
              "2    Test526.jpg  ...  Negative\n",
              "6    Test523.jpg  ...  Negative\n",
              "8   Test1071.jpg  ...  Positive\n",
              "11   Test668.jpg  ...  Negative\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm_qaDpazyZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7959f431-0908-4d26-cf83-aabff0e59f70"
      },
      "source": [
        "result=pd.concat([train_data1,test])\n",
        "result.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>length</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test660.jpg</td>\n",
              "      <td>Se aad\\n\\nate\\nCB LSU\\neer ee a\\nSenn\\n\\n2g) M...</td>\n",
              "      <td>[se, aad, ate, cb, lsu, eer, ee, senn, g, hy, ...</td>\n",
              "      <td>14</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test526.jpg</td>\n",
              "      <td>hLedemhenia\\nLeslee teriebenel\\nPe arcleneteal...</td>\n",
              "      <td>[hledemhenia, leslee, teriebenel, pe, arclenet...</td>\n",
              "      <td>12</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Test523.jpg</td>\n",
              "      <td>Cle) Dy\\nAi GAY\\n\\nera Swan</td>\n",
              "      <td>[cle, dy, ai, gay, era, swan]</td>\n",
              "      <td>6</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Test1071.jpg</td>\n",
              "      <td>ees MU WALLA OL\\n\\nem eae ie\\n\\nnot living by ...</td>\n",
              "      <td>[ee, mu, walla, ol, em, eae, ie, living, socie...</td>\n",
              "      <td>20</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Test668.jpg</td>\n",
              "      <td>@\\n\\nX/hen it come:\\nto the rights of\\n\\nthe U...</td>\n",
              "      <td>[x, hen, come, right, un, anything, neutral, em]</td>\n",
              "      <td>8</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        filename  ... sentiment\n",
              "1    Test660.jpg  ...  Negative\n",
              "2    Test526.jpg  ...  Negative\n",
              "6    Test523.jpg  ...  Negative\n",
              "8   Test1071.jpg  ...  Positive\n",
              "11   Test668.jpg  ...  Negative\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctv5Z28q1bWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c4e6a03-c6f3-4edb-a113-011412a1a602"
      },
      "source": [
        "# result=result.sort_values(by='filename')\n",
        "result.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(239, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GEC-R-GQOFk",
        "colab_type": "text"
      },
      "source": [
        "**Training using 1.6 million tweets from kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS8LvF_23Gey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cb3e16f3-df82-403c-e9fa-45814f9aaf4a"
      },
      "source": [
        "! git clone https://github.com/johnksander/twitter_NLP_analysis.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'twitter_NLP_analysis'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhMMp0UixPZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "332bd834-94c0-48fe-c710-d326f51897ea"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/  Dataset.zip  easy_ocr.csv  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtwitter_NLP_analysis\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ebYejExWez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d1d6238d-0a1f-48c7-948b-f8975f1352cb"
      },
      "source": [
        "ls -lrth twitter_NLP_analysis/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 82M\n",
            "-rwxr-xr-x 1 root root  347 Jul 13 10:22 \u001b[0m\u001b[01;32mREADME.md\u001b[0m*\n",
            "-rwxr-xr-x 1 root root  635 Jul 13 10:22 \u001b[01;32mMNB_output.txt\u001b[0m*\n",
            "-rwxr-xr-x 1 root root 2.3K Jul 13 10:22 \u001b[01;32mbuildWordVec.py\u001b[0m*\n",
            "-rwxr-xr-x 1 root root  42K Jul 13 10:22 \u001b[01;32mbetas.png\u001b[0m*\n",
            "drwxr-xr-x 3 root root 4.0K Jul 13 10:22 \u001b[01;34mdata\u001b[0m/\n",
            "-rwxr-xr-x 1 root root  329 Jul 13 10:22 \u001b[01;32mlasso_output.txt\u001b[0m*\n",
            "-rwxr-xr-x 1 root root 7.5K Jul 13 10:22 \u001b[01;32mfeature_betas.txt\u001b[0m*\n",
            "-rwxr-xr-x 1 root root 2.9K Jul 13 10:22 \u001b[01;32mrunMNB.py\u001b[0m*\n",
            "-rwxr-xr-x 1 root root  82M Jul 13 10:22 \u001b[01;32mtraining.1600000.processed.noemoticon.csv.zip\u001b[0m*\n",
            "-rwxr-xr-x 1 root root 5.0K Jul 13 10:22 \u001b[01;32mtweet_lasso.py\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A7LZh6Sxjqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp twitter_NLP_analysis/training.1600000.processed.noemoticon.csv.zip /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsaSphmaxp-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0dc6a778-0545-4eaf-e856-91db39898c33"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/     easy_ocr.csv  \u001b[01;32mtraining.1600000.processed.noemoticon.csv.zip\u001b[0m*\n",
            "Dataset.zip  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtwitter_NLP_analysis\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsLTt4ImjzRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mv training.1600000.processed.noemoticon.csv.zip tweets.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DyTrunD1R4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b9e0536f-a0c9-40db-d2c7-c3d39e7ae433"
      },
      "source": [
        "!unzip tweets.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  tweets.csv.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37t5h3Ey1mTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "384ea817-3c9e-4acd-fb9b-418bb7887d18"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/      \u001b[01;34m__MACOSX\u001b[0m/                                   \u001b[01;32mtweets.csv.zip\u001b[0m*\n",
            "Dataset.zip   \u001b[01;34msample_data\u001b[0m/                                \u001b[01;34mtwitter_NLP_analysis\u001b[0m/\n",
            "easy_ocr.csv  \u001b[01;32mtraining.1600000.processed.noemoticon.csv\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw4yuZe31w-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mv training.1600000.processed.noemoticon.csv tweets.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnaE-KYh16pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('tweets.csv',encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s23Gn3ID1_fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.rename(columns={'0':'target',\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\" : 'text'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2y-FhEC2TVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.drop(columns=['1467810369',\"Mon Apr 06 22:19:45 PDT 2009\",'NO_QUERY','_TheSpecialOne_'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA58vv0P4GOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b5b9850a-90b7-4c9e-f0dd-ba1327a5bb7c"
      },
      "source": [
        "data.target.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    799999\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJy_kIB4jANA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4a114374-bb6e-4ae3-bdbc-8a7dfef8eb47"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target                                               text\n",
              "0       0  is upset that he can't update his Facebook by ...\n",
              "1       0  @Kenichan I dived many times for the ball. Man...\n",
              "2       0    my whole body feels itchy and like its on fire \n",
              "3       0  @nationwideclass no, it's not behaving at all....\n",
              "4       0                      @Kwesidei not the whole crew "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYvcFwuHne8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ea9b286d-4740-43dc-8447-64ca320d9d04"
      },
      "source": [
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def clean_sentences(df):\n",
        "    reviews = []\n",
        "\n",
        "    for sent in (df['text']):\n",
        "        \n",
        "        #remove html content\n",
        "        review_text = BeautifulSoup(sent).get_text()\n",
        "        \n",
        "        #remove non-alphabetic characters\n",
        "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "        #tokenize the sentences\n",
        "        words = word_tokenize(review_text.lower())\n",
        "        \n",
        "        stops = set(stopwords.words(\"english\"))                  \n",
        "    # \n",
        "        # 5. Remove stop words\n",
        "        meaningful_words = [w for w in words if not w in stops]\n",
        "    \n",
        "        #lemmatize each word to its lemma\n",
        "        lemma_words = [lemmatizer.lemmatize(i) for i in meaningful_words]\n",
        "    \n",
        "        reviews.append(lemma_words)\n",
        "\n",
        "    return(reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGF4BNsY3PK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9598f213-e93b-497f-b80c-33c5f03e519e"
      },
      "source": [
        "data['clean_text']=clean_sentences(data)\n",
        "data.head()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "      <td>[nationwideclass, behaving, mad, see]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "      <td>[kwesidei, whole, crew]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                         clean_text\n",
              "0       0  ...  [upset, update, facebook, texting, might, cry,...\n",
              "1       0  ...  [kenichan, dived, many, time, ball, managed, s...\n",
              "2       0  ...             [whole, body, feel, itchy, like, fire]\n",
              "3       0  ...              [nationwideclass, behaving, mad, see]\n",
              "4       0  ...                            [kwesidei, whole, crew]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdQKVsUvUrkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f8eda87b-a06d-4aa3-f57c-4b5b09bc4b73"
      },
      "source": [
        "tokenized_tweets=data['clean_text']\n",
        "tokenized_tweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [upset, update, facebook, texting, might, cry,...\n",
              "1    [kenichan, dived, many, time, ball, managed, s...\n",
              "2               [whole, body, feel, itchy, like, fire]\n",
              "3                [nationwideclass, behaving, mad, see]\n",
              "4                              [kwesidei, whole, crew]\n",
              "Name: clean_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDE7zJRbK_IQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8c6ea952-768e-43b4-aa1d-e422c09e0943"
      },
      "source": [
        "data['target']=data['target'].replace(4,1)\n",
        "data['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    799999\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHdKu0YATFZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "105d6360-bf1a-45db-b06b-1a91e15c5f57"
      },
      "source": [
        "'''dataset['preprocess_text']\n",
        "\n",
        "allWords = ' '.join([text for text in dataset['preprocessed_text']])  # it must be string not tokens\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(allWords)\n",
        "plt.figure(figsize=(10, 10)) \n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
        "plt.axis('off') \n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'dataset[\\'preprocess_text\\']\\n\\nallWords = \\' \\'.join([text for text in dataset[\\'preprocessed_text\\']])  # it must be string not tokens\\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(allWords)\\nplt.figure(figsize=(10, 10)) \\nplt.imshow(wordcloud, interpolation=\"bilinear\") \\nplt.axis(\\'off\\') \\nplt.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCPSFBV7cegk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "# from tensorflow import \n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM,SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0BXThCJbeh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.utils import to_categorcial\n",
        "\n",
        "target=data.target.values\n",
        "y_target=to_categorical(target)\n",
        "num_classes=y_target.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSFADaldV2_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20a6773d-a7d0-4716-cfbc-f8f47c15096b"
      },
      "source": [
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YTP9wJtcBxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_val,Y_train,Y_val=train_test_split(tokenized_tweets,y_target,test_size=0.2,stratify=y_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTkQrZJ6b_a_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f3db223-be9c-498d-ce28-1c72a4928cdd"
      },
      "source": [
        "unique_words = set()\n",
        "len_max = 0\n",
        "\n",
        "for sent in (text_data['clean_sentence']):\n",
        "    \n",
        "    unique_words.update(sent)\n",
        "    \n",
        "    if(len_max<len(sent)):\n",
        "        len_max = len(sent)\n",
        "        \n",
        "#length of the list of unique_words gives the no of unique words\n",
        "print(len(list(unique_words)))\n",
        "print(len_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL3ddnjm2Au9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "85c4fdd0-783c-485f-e996-13b6f619faf6"
      },
      "source": [
        "text_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "      <th>clean_sentence</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test751.jpg</td>\n",
              "      <td>loses$,slo@</td>\n",
              "      <td>2</td>\n",
              "      <td>loses  slo</td>\n",
              "      <td>-0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test100.jpg</td>\n",
              "      <td>Married,uith,PRIDE</td>\n",
              "      <td>3</td>\n",
              "      <td>married uith pride</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test533.jpg</td>\n",
              "      <td>Mretahen,drnial,Leinactir,ondvaoowz,Caughi sha...</td>\n",
              "      <td>8</td>\n",
              "      <td>mretahen drnial leinactir ondvaoowz caughi sha...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Test3827.jpg</td>\n",
              "      <td>Being,gayis nota crime,anditis nota sin. Stop ...</td>\n",
              "      <td>13</td>\n",
              "      <td>being gayis nota crime anditis nota sin  stop ...</td>\n",
              "      <td>0.388889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Test211.jpg</td>\n",
              "      <td>HAT</td>\n",
              "      <td>1</td>\n",
              "      <td>hat</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       filename  ...  Category\n",
              "2   Test751.jpg  ... -0.300000\n",
              "3   Test100.jpg  ...  0.250000\n",
              "4   Test533.jpg  ...  0.000000\n",
              "6  Test3827.jpg  ...  0.388889\n",
              "7   Test211.jpg  ...  0.000000\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbzjS_WVcNSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "# tokenizer.fit_on_texts(list(X_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu6U0k6LcRZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "768ac860-2e7a-4bf7-8563-39cf29aee646"
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "# X_test = tokenizer.texts_to_sequences(text_data['clean_sentence'])\n",
        "\n",
        "#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n",
        "#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train,maxlen=len_max)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
        "# X_test = sequence.pad_sequences(X_test, maxlen=160)\n",
        "\n",
        "print(X_train.shape,X_val.shape)\n",
        "# print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1279999, 798) (320000, 798)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-U_R64jcV9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zPGVd6GLW_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "262e84a4-fac1-4fd9-836b-101f6a58fe57"
      },
      "source": [
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJJROl4c5WD",
        "colab_type": "text"
      },
      "source": [
        "below we define an Embedding layer with a vocabulary of **len(list(unique_words)** \n",
        "(e.g. integer encoded words from 0 to n, inclusive), a vector space of **300 ** \n",
        "dimensions in which words will be embedded, and input documents that have \n",
        "**len_max** words each"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjv8K8ylLDf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "c9048cc4-e2b2-49b7-cfe3-12afb7e29757"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
        "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "# model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "#model.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(num_classes,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.01),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 798, 300)          8100      \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 798, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 798, 128)          219648    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               6500      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 283,858\n",
            "Trainable params: 283,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwFe_e5CLeL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "1bfe6d46-8780-4a11-cf7e-6eb80aa55f95"
      },
      "source": [
        "%%time\n",
        "history=model.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=1, batch_size=1024, verbose=1, callbacks=callbacks)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-4b6d082ef276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history=model.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=1, batch_size=1024, verbose=1, callbacks=callbacks)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[1024,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/lstm_2/while/body/_1/lstm_cell_2/dropout_2/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/sequential_1/embedding_1/embedding_lookup/Reshape/_120]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[1024,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/lstm_2/while/body/_1/lstm_cell_2/dropout_2/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_16353]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyhr_4G01oOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sentiment(score):\n",
        "    return \"Positive\" if score>0.5 else \"Negative\"\n",
        "\n",
        "\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmqvOUv4MMHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a1c74e22-7012-460b-fddd-d8a0da3c3e67"
      },
      "source": [
        "# pred=model.predict_classes(X_test)\n",
        "# text_data['Category']=pred\n",
        "text_data['Category'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    118\n",
              "0     46\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2sxOAO8ML04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "94fb9d10-0040-4266-dcf8-877580bd3935"
      },
      "source": [
        "text_data['Category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    164\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fznYLVFQMLlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKmUxbeMDoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the weights\n",
        "model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylahTlkjC90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "070b1178-de8f-4efa-c463-2b1e8a970f1c"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;32mtweets.csv.zip\u001b[0m*\n",
            "\u001b[01;34m__MACOSX\u001b[0m/     \u001b[01;32mtweets.csv\u001b[0m*   \u001b[01;34mtwitter_NLP_analysis\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOnvIoxVjJ6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "adc9728d-1db3-4543-8d1b-940149ab2905"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQd_HVCAjhbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a49242cd-88b1-4e7b-9db7-273e693f1f2f"
      },
      "source": [
        "!ls -lrth\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 309M\n",
            "-rwxr-xr-x 1 root root 228M Dec 20  2017 tweets.csv\n",
            "drwxrwxr-x 2 root root 4.0K Dec 20  2017 __MACOSX\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 16:26 sample_data\n",
            "drwxr-xr-x 4 root root 4.0K Jul  7 06:41 twitter_NLP_analysis\n",
            "-rwxr-xr-x 1 root root  82M Jul  7 06:41 tweets.csv.zip\n",
            "drwxr-xr-x 2 root root 4.0K Jul  7 09:12 checkpoints\n",
            "drwxr-xr-x 3 root root 4.0K Jul  7 09:14 saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlbfMRkCjtoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_model.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dImI0V4bkVG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}